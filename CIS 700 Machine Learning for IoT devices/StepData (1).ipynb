{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiseDF = pickle.load( open( \"waist_final_9306300.pkl\", \"rb\" ))\n",
    "wristDF = pickle.load( open( \"wrist_final_9891800.pkl\", \"rb\" ))\n",
    "otherDF = pickle.load( open( \"others.pkl\", \"rb\" ))\n",
    "drivingDF = pickle.load( open( \"dirving_final_4800000.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-cc4a5473a6ae>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-cc4a5473a6ae>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    def __init__(self, X1,X2,Y):\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X1,X2,Y):\n",
    "        'Initialization'\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x1 = self.X1[index]\n",
    "        x2 = self.X1[index]\n",
    "        y = self.Y[index]\n",
    "\n",
    "        return x1,x2,y\n",
    "    \n",
    "training_set = Dataset(X_trainWaist,X_trainWrist, Y_train)\n",
    "training_generator = data.DataLoader(training_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting X train and Y train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLabel = wristDF[['x','y','z']]\n",
    "#YLabel = wristDF[['walk']]\n",
    "XLabel = waiseDF[['x','y','z']]\n",
    "YLabel = waiseDF[['walk']]\n",
    "\n",
    "xd = drivingDF[['x','y','z']]\n",
    "yd = drivingDF[['walk']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reshapping the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = XLabel.values.reshape(98918,100,3)\n",
    "#Y_train = YLabel.values.reshape(98918,100,1)\n",
    "X_train = XLabel.values.reshape(93063,100,3)\n",
    "Y_train = YLabel.values.reshape(93063,100,1)\n",
    "Xd_train = xd.values.reshape(48000,100,3)\n",
    "Yd_train = yd.values.reshape(48000,100,1)\n",
    "XOther_train = otherDF\n",
    "YOther_train = np.full((48000,100,1),0)\n",
    "#Y_train = max(z) for z in Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 100, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 100, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train,Xd_train,XOther_train])\n",
    "Y_train = np.concatenate([Y_train,Yd_train,YOther_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training, validation and test split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X_train, Y_train, test_size = 0.1)\n",
    "test_x, val_x, test_y, val_y = train_test_split(X_train, Y_train, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170156, 100, 3) (170156, 100, 1)\n",
      "(94532, 100, 3) (94532, 100, 1)\n",
      "(94531, 100, 3) (94531, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape)\n",
    "print(val_x.shape,val_y.shape)\n",
    "print(test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = train_x.reshape(175426,1,100,3)\n",
    "#Y_train = train_y.reshape(175426,100)\n",
    "X_train = train_x.reshape(170156,1,100,3)\n",
    "Y_train = train_y.reshape(170156,100)\n",
    "\n",
    "\n",
    "Y_train = np.array([max(z) for z in Y_train])\n",
    "\n",
    "#X_val = val_x.reshape(97459,1,100,3)\n",
    "#Y_val = val_y.reshape(97459,100)\n",
    "X_val = val_x.reshape(94532,1,100,3)\n",
    "Y_val = val_y.reshape(94532,100)\n",
    "Y_val = np.array([max(z) for z in Y_val])\n",
    "\n",
    "#X_test = test_x.reshape(97459,1,100,3)\n",
    "#Y_test = test_y.reshape(97459,100)\n",
    "X_test = test_x.reshape(94531,1,100,3)\n",
    "Y_test = test_y.reshape(94531,100)\n",
    "\n",
    "Y_test = np.array([max(z) for z in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170156, 1, 100, 3) (170156,)\n",
      "(94532, 1, 100, 3) (94532,)\n",
      "(94531, 1, 100, 3) (94531,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170156, 1, 100, 3]) torch.Size([170156])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.from_numpy(X_val)\n",
    "Y_val = torch.from_numpy(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([94532, 1, 100, 3]) torch.Size([94532])\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(X_test)\n",
    "Y_test = torch.from_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([94531, 1, 100, 3]) torch.Size([94531])\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,'num_workers': 6}\n",
    "max_epochs = 10\n",
    "\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(X_train, Y_train)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(X_val, Y_val)\n",
    "validation_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=(3,3), stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            #nn.Dropout(p=0.4),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "            \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            #nn.Dropout(p=0.4),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2,padding=1))\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2,padding=1))\n",
    "            \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            #nn.Dropout(p=0.4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        #self.fc1 = nn.Linear(100*2*4,64)\n",
    "        #self.fc2 = nn.Linear(64,2)\n",
    "        self.fc1 = nn.Linear(100*2*4,64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=(3,3), stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            #nn.Dropout(p=0.2),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=1,padding=1))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(32, 16, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.fc1 = nn.Linear(100*2*4,32)\n",
    "        self.fc2 = nn.Linear(32,2)\n",
    "        #self.fc1 = nn.Linear(100*2*4,128)\n",
    "        #self.fc2 = nn.Linear(128,2)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=800, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "#model=model.Dropout(p=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = Adam(model.parameters(), lr=0.07)\n",
    "optimizer = SGD(model.parameters(), lr=0.01)\n",
    "#learning_rate = 0.01\n",
    "if torch.cuda.is_available():\n",
    "    #model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model, (1,100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    #x_train, y_train = Variable(X_train), Variable(Y_train)\n",
    "    #x_val, y_val = Variable(X_val), Variable(Y_val)\n",
    "    for epoch in range(n_epochs):\n",
    "        for local_batch, local_labels in training_generator:\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            \n",
    "            output_train = model(local_batch.float())\n",
    "            loss_train = criterion(output_train, local_labels)\n",
    "            train_losses.append(loss_train)\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss = loss_train.item()\n",
    "\n",
    "        print(tr_loss)\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_train)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "                local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "                \n",
    "                output_val = model(local_batch.float())\n",
    "                loss_val = criterion(output_val, local_labels)\n",
    "                val_losses.append(loss_val)\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)\n",
    "        if(epoch==20):\n",
    "            torch.save(model, \"10stepdatamodel_epoch20_3layers_2.pt\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9919034242630005\n",
      "Epoch :  1 \t loss : tensor(1.9919, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  1 \t loss : tensor(0.0164, device='cuda:0')\n",
      "0.562522828578949\n",
      "Epoch :  2 \t loss : tensor(0.5625, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  2 \t loss : tensor(0.2224, device='cuda:0')\n",
      "0.6651625633239746\n",
      "Epoch :  3 \t loss : tensor(0.6652, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  3 \t loss : tensor(0.9713, device='cuda:0')\n",
      "1.7089930772781372\n",
      "Epoch :  4 \t loss : tensor(1.7090, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  4 \t loss : tensor(0.0017, device='cuda:0')\n",
      "5.540782451629639\n",
      "Epoch :  5 \t loss : tensor(5.5408, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  5 \t loss : tensor(0., device='cuda:0')\n",
      "1.3374497890472412\n",
      "Epoch :  6 \t loss : tensor(1.3374, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  6 \t loss : tensor(2.4041, device='cuda:0')\n",
      "3.5980300903320312\n",
      "Epoch :  7 \t loss : tensor(3.5980, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  7 \t loss : tensor(0., device='cuda:0')\n",
      "0.7667958736419678\n",
      "Epoch :  8 \t loss : tensor(0.7668, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  8 \t loss : tensor(0.0138, device='cuda:0')\n",
      "3.6122329235076904\n",
      "Epoch :  9 \t loss : tensor(3.6122, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  9 \t loss : tensor(0., device='cuda:0')\n",
      "2.1048882007598877\n",
      "Epoch :  10 \t loss : tensor(2.1049, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  10 \t loss : tensor(3.9981, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"waist_stepdatamodel_epoch10_3layers_SGD.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualization of train and validation loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "#plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating training accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.4, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=800, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = torch.load(\"stepdatamodel_epoch10_3layers_dropout_SGD.pt\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        output.append(model(local_batch.float().cuda()))\n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions1 = np.argmax(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49215425844519145"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_train, predictions1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Validation set accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in validation_generator:\n",
    "        output.append(model(local_batch.float().cuda()))\n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions2 = np.argmax(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49302881563914863"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_val, predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions2.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test set accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset(X_test, Y_test)\n",
    "testset_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in testset_generator:\n",
    "        output.append(model(local_batch.float().cuda()))\n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions3 = np.argmax(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_score(Y_test, predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions3.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
