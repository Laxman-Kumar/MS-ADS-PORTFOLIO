{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Prototypical Learning - Combined.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"GGYB4iSpSZJZ"},"source":["from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","from torchsummary import summary\n","import pickle\n","import os\n","from functions import Dataset,prototypical_loss as loss_fn ,ProtoNet#,PrototypicalBatchSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0o--h56lSZJf"},"source":["### Waist"]},{"cell_type":"code","metadata":{"id":"TkX8uciUSZJg","outputId":"1651317a-45f3-4f84-838c-91e246a24331"},"source":["#waistDF = pickle.load( open( \"waist_final_10272800_new.pkl\", \"rb\" ))\n","waistDF = pickle.load( open( \"waist_final_10272800_new_nozero.pkl\", \"rb\" ))\n","\n","XTrain = waistDF[['x','y','z']]\n","YTrain = waistDF[['speed']]\n","\n","X_trainWaist = XTrain.values.reshape(102728,100,3)\n","Y_trainWaist = YTrain.values.reshape(102728,100,1)\n","Y_trainWaist = np.array([float(max(z)) for z in Y_trainWaist])\n","\n","waistTrain_x, waistVal_x, waistTrain_y, waistVal_y = train_test_split(X_trainWaist, Y_trainWaist, test_size = 0.1,random_state=100)\n","waistTest_x, waistVal_x, waistTest_y, waistVal_y = train_test_split(waistVal_x, waistVal_y, test_size = 0.5,random_state=100)\n","print(len(waistTrain_x))\n","print(len(waistTest_x))\n","print(len(waistVal_x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["92455\n","5136\n","5137\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LFCEpvAsSZJh","outputId":"ebfedbba-fdd0-4a3e-c82e-56e82c87b522"},"source":["waistTrain_x = waistTrain_x.reshape(92455,1,100,3)\n","waistTest_x = waistTest_x.reshape(5136,1,100,3)\n","waistVal_x = waistVal_x.reshape(5137,1,100,3)\n","print(waistTrain_x.shape)#,train_y.shape)\n","print(waistTest_x.shape)#,test_y.shape)\n","print(waistVal_x.shape)#,val_y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(92455, 1, 100, 3)\n","(5136, 1, 100, 3)\n","(5137, 1, 100, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rl2tAgE7SZJh"},"source":["waistTrain_x = torch.from_numpy(waistTrain_x)\n","waistTrain_y = torch.from_numpy(waistTrain_y)\n","\n","waistTest_x = torch.from_numpy(waistTest_x)\n","waistTest_y = torch.from_numpy(waistTest_y)\n","\n","waistVal_x = torch.from_numpy(waistVal_x)\n","waistVal_y = torch.from_numpy(waistVal_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DJph-NQkSZJh"},"source":["### Wrist"]},{"cell_type":"code","metadata":{"id":"dK8GTc7vSZJi","outputId":"9eaba1d1-c508-4f5b-bf3e-4ad53cd532cd"},"source":["#wristDF = pickle.load( open( \"wrist_final_10272800_new.pkl\", \"rb\" ))\n","wristDF = pickle.load( open( \"wrist_final_10272800_new_nozero.pkl\", \"rb\" ))\n","\n","XTrain = wristDF[['x','y','z']]\n","YTrain = wristDF[['speed']]\n","\n","X_trainWrist = XTrain.values.reshape(102728,100,3)\n","Y_trainWrist = YTrain.values.reshape(102728,100,1)\n","Y_trainWrist = np.array([float(max(z)) for z in Y_trainWrist])\n","\n","wristTrain_x, wristVal_x, wristTrain_y, wristVal_y = train_test_split(X_trainWrist, Y_trainWrist, test_size = 0.1,random_state=100)\n","wristTest_x, wristVal_x, wristTest_y, wristVal_y = train_test_split(wristVal_x, wristVal_y, test_size = 0.5,random_state=100)\n","print(len(wristTrain_x))\n","print(len(wristTest_x))\n","print(len(wristVal_x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["92455\n","5136\n","5137\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h_eTHe1mSZJi","outputId":"3a62bcbb-902c-40a3-8584-b90bcb8f5bae"},"source":["wristTrain_x = wristTrain_x.reshape(92455,1,100,3)\n","wristTest_x = wristTest_x.reshape(5136,1,100,3)\n","wristVal_x = wristVal_x.reshape(5137,1,100,3)\n","print(wristTrain_x.shape)#,train_y.shape)\n","print(wristTest_x.shape)#,test_y.shape)\n","print(wristVal_x.shape)#,val_y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(92455, 1, 100, 3)\n","(5136, 1, 100, 3)\n","(5137, 1, 100, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OG3Utbv9SZJi"},"source":["wristTrain_x = torch.from_numpy(wristTrain_x)\n","wristTrain_y = torch.from_numpy(wristTrain_y)\n","\n","wristTest_x = torch.from_numpy(wristTest_x)\n","wristTest_y = torch.from_numpy(wristTest_y)\n","\n","wristVal_x = torch.from_numpy(wristVal_x)\n","wristVal_y = torch.from_numpy(wristVal_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBbw9jT1SZJj","outputId":"6829b2b8-295a-4407-dff5-7137ae4f5d4c"},"source":["#Classes\n","print(len(np.unique(wristTrain_y)),len(np.unique(wristTest_y)),len(np.unique(wristVal_y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["14 14 14\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d-vmBFXCSZJj"},"source":["### DataSet Class"]},{"cell_type":"code","metadata":{"id":"paiKJsQLSZJj"},"source":["training_set = Dataset(wristTrain_x,waistTrain_x, wristTrain_y)\n","test_set = Dataset(wristTest_x,waistTest_x, wristTest_y)\n","val_set = Dataset(wristVal_x,waistVal_x, wristVal_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTTNtybcSZJk"},"source":["x = ['dataset_root','experiment_root','cuda']\n","\n","classes_per_it_tr = 3\n","classes_per_it_val =3\n","classes_per_it_tr = 3\n","\n","manual_seeds =7\n","iterations = 10\n","epochs = 100\n","learning_rate = 0.01\n","lr_scheduler_step = 0.1\n","lr_scheduler_gamma = 0.5\n","cuda = 0\n","num_support_tr = 40\n","num_query_tr = 40\n","num_support_val = 40\n","num_query_val = 40"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LoRCfn-JSZJk"},"source":["class PrototypicalBatchSampler(object):\n","    '''\n","    PrototypicalBatchSampler: yield a batch of indexes at each iteration.\n","    Indexes are calculated by keeping in account 'classes_per_it' and 'num_samples',\n","    In fact at every iteration the batch indexes will refer to  'num_support' + 'num_query' samples\n","    for 'classes_per_it' random classes.\n","\n","    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n","    '''\n","\n","    def __init__(self, labels, classes_per_it, num_samples, iterations):\n","        '''\n","        Initialize the PrototypicalBatchSampler object\n","        Args:\n","        - labels: an iterable containing all the labels for the current dataset\n","        samples indexes will be infered from this iterable.\n","        - classes_per_it: number of random classes for each iteration\n","        - num_samples: number of samples for each iteration for each class (support + query)\n","        - iterations: number of iterations (episodes) per epoch\n","        '''\n","        super(PrototypicalBatchSampler, self).__init__()\n","        self.labels = labels\n","        self.labels = [float(x) for x in self.labels]\n","        self.classes_per_it = classes_per_it\n","        self.sample_per_class = num_samples\n","        self.iterations = iterations\n","\n","        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n","        self.classes = [float(x) for x in self.classes]\n","        self.classes = torch.FloatTensor(self.classes)\n","        #print(self.classes)\n","        # create a matrix, indexes, of dim: classes X max(elements per class)\n","        # fill it with nans\n","        # for every class c, fill the relative row with the indices samples belonging to c\n","        # in numel_per_class we store the number of samples for each class/row\n","        self.idxs = range(len(self.labels))\n","        self.indexes = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n","        self.indexes = torch.Tensor(self.indexes)\n","        self.numel_per_class = torch.zeros_like(self.classes)\n","        for idx, label in enumerate(self.labels):\n","\n","            label_idx = np.argwhere(self.classes == label).item()\n","            self.indexes[label_idx, np.where(np.isnan(self.indexes[label_idx]))[0][0]] = idx\n","            self.numel_per_class[label_idx] += 1\n","\n","    def __iter__(self):\n","        '''\n","        yield a batch of indexes\n","        '''\n","        \n","        spc = self.sample_per_class\n","        cpi = self.classes_per_it\n","        \n","        #print(spc)\n","        for it in range(self.iterations):\n","            batch_size = spc * cpi\n","            batch = torch.LongTensor(batch_size)\n","            c_idxs = torch.randperm(len(self.classes))[:cpi]\n","            for i, c in enumerate(self.classes[c_idxs]):\n","                s = slice(i * spc, (i + 1) * spc)\n","                # FIXME when torch.argwhere will exists\n","                label_idx = torch.arange(len(self.classes)).long()[self.classes == c].item()\n","                sample_idxs = torch.randperm(int(self.numel_per_class[label_idx]))[:spc]\n","                batch[s] = self.indexes[label_idx][sample_idxs]\n","            batch = batch[torch.randperm(len(batch))]\n","            yield batch\n","\n","    def __len__(self):\n","        '''\n","        returns the number of iterations (episodes) per epoch\n","        '''\n","        return self.iterations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQGmtk6OSZJl"},"source":["def init_seed(manual_seed):\n","    '''\n","    Disable cudnn to maximize reproducibility\n","    '''\n","    torch.cuda.cudnn_enabled = False\n","    np.random.seed(manual_seed)\n","    torch.manual_seed(manual_seed)\n","    torch.cuda.manual_seed(manual_seed)\n","    \n","def init_dataset(mode):\n","    if (mode == \"train\"):dataset = training_set\n","    if (mode == \"test\"):dataset = test_set\n","    if (mode == \"val\"):dataset = val_set\n","    n_classes = 14\n","    if n_classes < classes_per_it_tr or n_classes < classes_per_it_val:\n","        raise(Exception('There are not enough classes in the dataset in order ' +\n","                        'to satisfy the chosen classes_per_it. Decrease the ' +\n","                        'classes_per_it_{tr/val} option and try again.'))\n","    return dataset\n","\n","def init_sampler(labels, mode):\n","    if 'train' in mode:\n","        classes_per_it = classes_per_it_tr\n","        num_samples = num_support_tr + num_query_tr\n","    else:\n","        classes_per_it = classes_per_it_val\n","        num_samples = num_support_val + num_query_val\n","\n","    return PrototypicalBatchSampler(labels=labels,classes_per_it=classes_per_it,\n","                                    num_samples=num_samples,\n","                                    iterations=iterations)\n","\n","def init_dataloader(mode):\n","    dataset = init_dataset(mode)\n","    sampler = init_sampler(dataset.Y, mode)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVNFPpObSZJm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eYKJ-ZBiSZJm"},"source":["def init_protonet():\n","    device = 'cuda:0' if torch.cuda.is_available() and cuda else 'cpu'\n","    model = ProtoNet().to(device)\n","    return model\n","\n","def init_optim(model):return torch.optim.Adam(params=model.parameters(),lr=learning_rate)\n","\n","def init_lr_scheduler(optim):\n","    return torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=lr_scheduler_gamma,step_size=lr_scheduler_step)\n","\n","def save_list_to_file(path, thelist):\n","    with open(path, 'w') as f:\n","        for item in thelist:f.write(\"%s\\n\" % item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"de1-7OajSZJm"},"source":["import torch.nn as nn\n","from torch.nn import functional as F\n","\n","class Combine_Waist_Wrist(nn.Module):\n","    def __init__(self, modelA, modelB):\n","        super(Combine_Waist_Wrist, self).__init__()\n","        self.modelA = modelA\n","        self.modelB = modelB\n","        \n","        self.fc1 = nn.Linear(100*2*4*2,64)\n","        #self.fc1 = nn.Linear(100*2*2,64)\n","        \n","        self.classifier = nn.Linear(64,1)   ### 2 -> 1 , make labels 1 dimensions: 0 or 1\n","        #linear layers\n","        #linear layers\n","    def forward(self, x1, x2):\n","        x1 = self.modelA(x1)\n","        x2 = self.modelB(x2)\n","        out = torch.cat((x1, x2), dim=1)\n","        \n","        out = F.relu(self.fc1(out))\n","        #out = (self.fc1(out))\n","        x = self.classifier(out)  # not relu\n","        #x = F.tanh(out)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIOXap34SZJm"},"source":["def train(tr_dataloader, model, optim, lr_scheduler, val_dataloader=None):\n","    \n","    device = 'cuda:0' if torch.cuda.is_available() and cuda else 'cpu'\n","\n","    if val_dataloader is None:best_state = None\n","    \n","    train_loss = []\n","    train_acc = []\n","    val_loss = []\n","    val_acc = []\n","    best_acc = 0\n","\n","    best_model_path = 'best_model.pth'\n","    last_model_path = 'last_model.pth'\n","\n","    for epoch in range(epochs):\n","        print('=== Epoch: {} ==='.format(epoch))\n","        tr_iter = iter(tr_dataloader)\n","        model.train()\n","        for batch in tr_iter:\n","            optim.zero_grad()\n","            x1,x2, y = batch\n","            x1,x2, y = x1.to(device), x2.to(device),y.to(device)\n","            model_output = model(x1.float(),x2.float())\n","            loss,acc= loss_fn(model_output,target=y,n_support=num_support_tr)\n","            loss.backward()\n","            optim.step()\n","            train_loss.append(loss.item())\n","            train_acc.append(acc.item())\n","        avg_loss = np.mean(train_loss[-iterations:])\n","        avg_acc = np.mean(train_acc[-iterations:])\n","        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n","        lr_scheduler.step()\n","        if val_dataloader is None:\n","            continue\n","            \n","        val_iter = iter(val_dataloader)\n","        model.eval()\n","        \n","        for batch in val_iter:\n","            x1,x2, y = batch\n","            x1,x2, y = x1.to(device), x2.to(device),y.to(device)\n","            model_output = model(x1.float(),x2.float())\n","            loss, acc = loss_fn(model_output, target=y,n_support=num_support_val)\n","            val_loss.append(loss.item())\n","            val_acc.append(acc.item())\n","        avg_loss = np.mean(val_loss[-iterations:])\n","        avg_acc = np.mean(val_acc[-iterations:])\n","        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(best_acc)\n","        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(avg_loss, avg_acc, postfix))\n","        if avg_acc >= best_acc:\n","            torch.save(model.state_dict(), best_model_path)\n","            best_acc = avg_acc\n","            best_state = model.state_dict()\n","\n","    torch.save(model.state_dict(), last_model_path)\n","\n","    for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n","        save_list_to_file(os.path.join(name + '.txt'), locals()[name])\n","\n","    return best_state, best_acc, train_loss, train_acc, val_loss, val_acc\n","\n","\n","def test(test_dataloader, model):\n","    '''\n","    Test the model trained with the prototypical learning algorithm\n","    '''\n","    device = 'cuda:0' if torch.cuda.is_available() and cuda else 'cpu'\n","    avg_acc = list()\n","    for epoch in range(10):\n","        test_iter = iter(test_dataloader)\n","        for batch in test_iter:\n","            x1,x2, y = batch\n","            x1,x2, y = x1.to(device), x2.to(device),y.to(device)\n","            model_output = model(x1.float(),x2.float())\n","            _, acc = loss_fn(model_output, target=y,n_support=num_support_val)\n","            avg_acc.append(acc.item())\n","    avg_acc = np.mean(avg_acc)\n","    print('Test Acc: {}'.format(avg_acc))\n","\n","    return avg_acc\n","\n","\n","def eval():\n","    '''\n","    Initialize everything and train\n","    '''\n","    #options = get_parser().parse_args()\n","\n","    if torch.cuda.is_available() and not options.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","    init_seed()\n","    test_dataloader = init_dataset()[-1]\n","    model = init_protonet()\n","    model_path = os.path.join('best_model.pth')\n","    model.load_state_dict(torch.load(model_path))\n","\n","    test(opt=options,test_dataloader=test_dataloader,model=model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCTT_ORKSZJn"},"source":["def main():\n","  \n","    if torch.cuda.is_available() and not cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","    init_seed(manual_seeds)\n","    tr_dataloader = init_dataloader('train')\n","    val_dataloader = init_dataloader('val')\n","    # trainval_dataloader = init_dataloader(options, 'trainval')\n","    test_dataloader = init_dataloader('test')\n","\n","    #model = init_protonet()\n","    modelWaist = init_protonet()\n","    modelWrist = init_protonet()\n","    modelCombine = Combine_Waist_Wrist(modelWaist,modelWrist)\n","    optim = init_optim(modelCombine)\n","    \n","    lr_scheduler = init_lr_scheduler(optim)\n","    res = train(tr_dataloader=tr_dataloader,val_dataloader=val_dataloader,lr_scheduler=lr_scheduler,\n","                model=modelCombine,optim=optim)\n","    \n","    best_state, best_acc, train_loss, train_acc, val_loss, val_acc = res\n","    print('Testing with last model..')\n","    test(test_dataloader=test_dataloader,model=modelCombine)\n","\n","    modelCombine.load_state_dict(best_state)\n","    print('Testing with best model..')\n","    test(test_dataloader=test_dataloader,model=modelCombine)\n","\n","    # optim = init_optim(options, model)\n","    # lr_scheduler = init_lr_scheduler(options, optim)\n","\n","    # print('Training on train+val set..')\n","    # train(opt=options,tr_dataloader=trainval_dataloader,\n","    #       val_dataloader=None,model=model,optim=optim,lr_scheduler=lr_scheduler)\n","\n","    # print('Testing final model..')\n","    # test(opt=options,test_dataloader=test_dataloader,model=model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GpsTanHASZJo","outputId":"5e9cb1ed-8c7d-4c08-835f-c2cc2e0c35fb"},"source":["classes_per_it_tr = 3\n","classes_per_it_val =3\n","classes_per_it_tr = 3\n","\n","iterations = 10\n","epochs = 100\n","\n","num_support_tr = 40\n","num_query_tr = 40\n","num_support_val = 40\n","num_query_val = 40\n","\n","main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=== Epoch: 0 ===\n","Avg Train Loss: 1.1443148255348206, Avg Train Acc: 0.5883333355188369\n","Avg Val Loss: 0.7787049114704132, Avg Val Acc: 0.6225000113248825 (Best)\n","=== Epoch: 1 ===\n","Avg Train Loss: 0.64212886095047, Avg Train Acc: 0.6716666638851165\n","Avg Val Loss: 0.6412241250276566, Avg Val Acc: 0.7366666674613953 (Best)\n","=== Epoch: 2 ===\n","Avg Train Loss: 0.6531057864427566, Avg Train Acc: 0.7508333325386047\n","Avg Val Loss: 0.5308374851942063, Avg Val Acc: 0.7599999994039536 (Best)\n","=== Epoch: 3 ===\n","Avg Train Loss: 0.5599259823560715, Avg Train Acc: 0.760833340883255\n","Avg Val Loss: 0.6483340293169022, Avg Val Acc: 0.7258333325386047 (Best: 0.7599999994039536)\n","=== Epoch: 4 ===\n","Avg Train Loss: 0.6311839878559112, Avg Train Acc: 0.7166666626930237\n","Avg Val Loss: 0.6300419420003891, Avg Val Acc: 0.7166666686534882 (Best: 0.7599999994039536)\n","=== Epoch: 5 ===\n","Avg Train Loss: 0.511501032114029, Avg Train Acc: 0.776666671037674\n","Avg Val Loss: 0.45612139999866486, Avg Val Acc: 0.8299999952316284 (Best)\n","=== Epoch: 6 ===\n","Avg Train Loss: 0.6032148689031601, Avg Train Acc: 0.7450000047683716\n","Avg Val Loss: 0.49861747249960897, Avg Val Acc: 0.7558333277702332 (Best: 0.8299999952316284)\n","=== Epoch: 7 ===\n","Avg Train Loss: 0.48424513041973116, Avg Train Acc: 0.801666671037674\n","Avg Val Loss: 0.5396932661533356, Avg Val Acc: 0.7558333337306976 (Best: 0.8299999952316284)\n","=== Epoch: 8 ===\n","Avg Train Loss: 0.4471828371286392, Avg Train Acc: 0.8149999976158142\n","Avg Val Loss: 0.4673217207193375, Avg Val Acc: 0.7583333373069763 (Best: 0.8299999952316284)\n","=== Epoch: 9 ===\n","Avg Train Loss: 0.4597178801894188, Avg Train Acc: 0.7650000035762787\n","Avg Val Loss: 0.45664851665496825, Avg Val Acc: 0.7808333396911621 (Best: 0.8299999952316284)\n","=== Epoch: 10 ===\n","Avg Train Loss: 0.6155463457107544, Avg Train Acc: 0.739166671037674\n","Avg Val Loss: 0.4099926918745041, Avg Val Acc: 0.7883333325386047 (Best: 0.8299999952316284)\n","=== Epoch: 11 ===\n","Avg Train Loss: 0.4903048127889633, Avg Train Acc: 0.7683333396911621\n","Avg Val Loss: 0.49684531688690187, Avg Val Acc: 0.7858333349227905 (Best: 0.8299999952316284)\n","=== Epoch: 12 ===\n","Avg Train Loss: 0.5887526035308838, Avg Train Acc: 0.7208333343267441\n","Avg Val Loss: 0.6298582077026367, Avg Val Acc: 0.6175000041723251 (Best: 0.8299999952316284)\n","=== Epoch: 13 ===\n","Avg Train Loss: 0.4697571963071823, Avg Train Acc: 0.7499999880790711\n","Avg Val Loss: 0.5562514066696167, Avg Val Acc: 0.745833334326744 (Best: 0.8299999952316284)\n","=== Epoch: 14 ===\n","Avg Train Loss: 0.4857200786471367, Avg Train Acc: 0.7924999952316284\n","Avg Val Loss: 0.4373592808842659, Avg Val Acc: 0.7925000011920929 (Best: 0.8299999952316284)\n","=== Epoch: 15 ===\n","Avg Train Loss: 0.4596170485019684, Avg Train Acc: 0.7624999940395355\n","Avg Val Loss: 0.4807781308889389, Avg Val Acc: 0.8066666662693024 (Best: 0.8299999952316284)\n","=== Epoch: 16 ===\n","Avg Train Loss: 0.404742755740881, Avg Train Acc: 0.8183333396911621\n","Avg Val Loss: 0.5286260142922401, Avg Val Acc: 0.8108333349227905 (Best: 0.8299999952316284)\n","=== Epoch: 17 ===\n","Avg Train Loss: 0.5539053082466125, Avg Train Acc: 0.746666669845581\n","Avg Val Loss: 0.6293542712926865, Avg Val Acc: 0.6433333247900009 (Best: 0.8299999952316284)\n","=== Epoch: 18 ===\n","Avg Train Loss: 0.678112581372261, Avg Train Acc: 0.602499994635582\n","Avg Val Loss: 0.65207140147686, Avg Val Acc: 0.6358333349227905 (Best: 0.8299999952316284)\n","=== Epoch: 19 ===\n","Avg Train Loss: 0.6704666465520859, Avg Train Acc: 0.7216666638851166\n","Avg Val Loss: 0.42496476173400877, Avg Val Acc: 0.8000000059604645 (Best: 0.8299999952316284)\n","=== Epoch: 20 ===\n","Avg Train Loss: 0.3903036430478096, Avg Train Acc: 0.8083333253860474\n","Avg Val Loss: 0.45386825799942015, Avg Val Acc: 0.8241666734218598 (Best: 0.8299999952316284)\n","=== Epoch: 21 ===\n","Avg Train Loss: 0.5503828197717666, Avg Train Acc: 0.7700000017881393\n","Avg Val Loss: 0.6149969145655632, Avg Val Acc: 0.7408333361148834 (Best: 0.8299999952316284)\n","=== Epoch: 22 ===\n","Avg Train Loss: 0.445630207657814, Avg Train Acc: 0.8050000011920929\n","Avg Val Loss: 0.3602492645382881, Avg Val Acc: 0.8316666662693024 (Best)\n","=== Epoch: 23 ===\n","Avg Train Loss: 0.41248629689216615, Avg Train Acc: 0.846666669845581\n","Avg Val Loss: 0.6192731648683548, Avg Val Acc: 0.7083333313465119 (Best: 0.8316666662693024)\n","=== Epoch: 24 ===\n","Avg Train Loss: 0.5590631902217865, Avg Train Acc: 0.7224999964237213\n","Avg Val Loss: 0.5509841799736023, Avg Val Acc: 0.7524999976158142 (Best: 0.8316666662693024)\n","=== Epoch: 25 ===\n","Avg Train Loss: 0.4317517578601837, Avg Train Acc: 0.8066666722297668\n","Avg Val Loss: 0.6068664491176605, Avg Val Acc: 0.714166671037674 (Best: 0.8316666662693024)\n","=== Epoch: 26 ===\n","Avg Train Loss: 0.549813874065876, Avg Train Acc: 0.7541666686534881\n","Avg Val Loss: 0.3830698773264885, Avg Val Acc: 0.8108333349227905 (Best: 0.8316666662693024)\n","=== Epoch: 27 ===\n","Avg Train Loss: 0.43272136226296426, Avg Train Acc: 0.8291666746139527\n","Avg Val Loss: 0.533913654088974, Avg Val Acc: 0.7716666638851166 (Best: 0.8316666662693024)\n","=== Epoch: 28 ===\n","Avg Train Loss: 0.5297591209411621, Avg Train Acc: 0.7658333301544189\n","Avg Val Loss: 0.5864124104380608, Avg Val Acc: 0.7558333337306976 (Best: 0.8316666662693024)\n","=== Epoch: 29 ===\n","Avg Train Loss: 0.5719142019748688, Avg Train Acc: 0.7374999940395355\n","Avg Val Loss: 0.45104877948760985, Avg Val Acc: 0.7991666734218598 (Best: 0.8316666662693024)\n","=== Epoch: 30 ===\n","Avg Train Loss: 0.44795354902744294, Avg Train Acc: 0.7841666638851166\n","Avg Val Loss: 0.3740112692117691, Avg Val Acc: 0.8816666781902314 (Best)\n","=== Epoch: 31 ===\n","Avg Train Loss: 0.5304024815559387, Avg Train Acc: 0.7741666615009308\n","Avg Val Loss: 0.40817151963710785, Avg Val Acc: 0.8324999988079071 (Best: 0.8816666781902314)\n","=== Epoch: 32 ===\n","Avg Train Loss: 0.4555182509124279, Avg Train Acc: 0.8150000035762787\n","Avg Val Loss: 0.472340327501297, Avg Val Acc: 0.7966666638851165 (Best: 0.8816666781902314)\n","=== Epoch: 33 ===\n","Avg Train Loss: 0.46012191772460936, Avg Train Acc: 0.7941666632890702\n","Avg Val Loss: 0.38225248605012896, Avg Val Acc: 0.862499988079071 (Best: 0.8816666781902314)\n","=== Epoch: 34 ===\n","Avg Train Loss: 0.3933495730161667, Avg Train Acc: 0.8433333277702332\n","Avg Val Loss: 0.41496822237968445, Avg Val Acc: 0.8324999868869781 (Best: 0.8816666781902314)\n","=== Epoch: 35 ===\n","Avg Train Loss: 0.41942146364599464, Avg Train Acc: 0.8241666734218598\n","Avg Val Loss: 0.44643888175487517, Avg Val Acc: 0.7800000041723252 (Best: 0.8816666781902314)\n","=== Epoch: 36 ===\n","Avg Train Loss: 0.32624711841344833, Avg Train Acc: 0.8658333361148834\n","Avg Val Loss: 0.4985699847340584, Avg Val Acc: 0.7949999928474426 (Best: 0.8816666781902314)\n","=== Epoch: 37 ===\n","Avg Train Loss: 0.3432061210274696, Avg Train Acc: 0.8949999928474426\n","Avg Val Loss: 0.5490864098072052, Avg Val Acc: 0.7883333325386047 (Best: 0.8816666781902314)\n","=== Epoch: 38 ===\n","Avg Train Loss: 0.4527233146131039, Avg Train Acc: 0.8324999988079071\n","Avg Val Loss: 0.511409830302, Avg Val Acc: 0.7550000011920929 (Best: 0.8816666781902314)\n","=== Epoch: 39 ===\n","Avg Train Loss: 0.5075930885970592, Avg Train Acc: 0.8041666805744171\n","Avg Val Loss: 0.543461212515831, Avg Val Acc: 0.7041666656732559 (Best: 0.8816666781902314)\n","=== Epoch: 40 ===\n","Avg Train Loss: 0.3190084010362625, Avg Train Acc: 0.8691666722297668\n","Avg Val Loss: 0.47724844291806223, Avg Val Acc: 0.8074999928474427 (Best: 0.8816666781902314)\n","=== Epoch: 41 ===\n","Avg Train Loss: 0.3603845279663801, Avg Train Acc: 0.8599999964237213\n","Avg Val Loss: 0.35391541868448256, Avg Val Acc: 0.8591666698455811 (Best: 0.8816666781902314)\n","=== Epoch: 42 ===\n","Avg Train Loss: 0.44406343847513197, Avg Train Acc: 0.8175000011920929\n","Avg Val Loss: 0.4270668238401413, Avg Val Acc: 0.7991666734218598 (Best: 0.8816666781902314)\n","=== Epoch: 43 ===\n","Avg Train Loss: 0.46962898671627046, Avg Train Acc: 0.7958333432674408\n","Avg Val Loss: 0.4625422403216362, Avg Val Acc: 0.802499994635582 (Best: 0.8816666781902314)\n","=== Epoch: 44 ===\n","Avg Train Loss: 0.31363574527204036, Avg Train Acc: 0.8566666722297669\n","Avg Val Loss: 0.3993389278650284, Avg Val Acc: 0.8375000059604645 (Best: 0.8816666781902314)\n","=== Epoch: 45 ===\n","Avg Train Loss: 0.4285893812775612, Avg Train Acc: 0.8283333420753479\n","Avg Val Loss: 0.3283266879618168, Avg Val Acc: 0.8525000035762786 (Best: 0.8816666781902314)\n","=== Epoch: 46 ===\n"],"name":"stdout"},{"output_type":"stream","text":["Avg Train Loss: 0.46107910126447677, Avg Train Acc: 0.7825000047683716\n","Avg Val Loss: 0.4499838650226593, Avg Val Acc: 0.85 (Best: 0.8816666781902314)\n","=== Epoch: 47 ===\n","Avg Train Loss: 0.5878714770078659, Avg Train Acc: 0.8008333265781402\n","Avg Val Loss: 0.46063866317272184, Avg Val Acc: 0.8233333349227905 (Best: 0.8816666781902314)\n","=== Epoch: 48 ===\n","Avg Train Loss: 0.4600615620613098, Avg Train Acc: 0.8358333349227905\n","Avg Val Loss: 0.481922821700573, Avg Val Acc: 0.7866666674613952 (Best: 0.8816666781902314)\n","=== Epoch: 49 ===\n","Avg Train Loss: 0.35651449859142303, Avg Train Acc: 0.860833328962326\n","Avg Val Loss: 0.3997245579957962, Avg Val Acc: 0.8308333396911621 (Best: 0.8816666781902314)\n","=== Epoch: 50 ===\n","Avg Train Loss: 0.3327326849102974, Avg Train Acc: 0.8791666686534881\n","Avg Val Loss: 0.28911475017666816, Avg Val Acc: 0.8791666626930237 (Best: 0.8816666781902314)\n","=== Epoch: 51 ===\n","Avg Train Loss: 0.4511170119047165, Avg Train Acc: 0.8024999976158143\n","Avg Val Loss: 0.4412672400474548, Avg Val Acc: 0.8275000035762787 (Best: 0.8816666781902314)\n","=== Epoch: 52 ===\n","Avg Train Loss: 0.3654028967022896, Avg Train Acc: 0.8533333361148834\n","Avg Val Loss: 0.5473525568842887, Avg Val Acc: 0.7808333396911621 (Best: 0.8816666781902314)\n","=== Epoch: 53 ===\n","Avg Train Loss: 0.2628276079893112, Avg Train Acc: 0.9133333384990692\n","Avg Val Loss: 0.4396165028214455, Avg Val Acc: 0.8058333456516266 (Best: 0.8816666781902314)\n","=== Epoch: 54 ===\n","Avg Train Loss: 0.41444296687841414, Avg Train Acc: 0.8258333325386047\n","Avg Val Loss: 0.45933782011270524, Avg Val Acc: 0.7924999952316284 (Best: 0.8816666781902314)\n","=== Epoch: 55 ===\n","Avg Train Loss: 0.44951536506414413, Avg Train Acc: 0.7966666638851165\n","Avg Val Loss: 0.3986313134431839, Avg Val Acc: 0.8258333384990693 (Best: 0.8816666781902314)\n","=== Epoch: 56 ===\n","Avg Train Loss: 0.4460896208882332, Avg Train Acc: 0.7983333468437195\n","Avg Val Loss: 0.41460627168416975, Avg Val Acc: 0.7991666615009307 (Best: 0.8816666781902314)\n","=== Epoch: 57 ===\n","Avg Train Loss: 0.33768634535372255, Avg Train Acc: 0.8641666650772095\n","Avg Val Loss: 0.6097033474594354, Avg Val Acc: 0.739999994635582 (Best: 0.8816666781902314)\n","=== Epoch: 58 ===\n","Avg Train Loss: 0.5300527170300484, Avg Train Acc: 0.7916666626930237\n","Avg Val Loss: 0.38634551391005517, Avg Val Acc: 0.8166666686534881 (Best: 0.8816666781902314)\n","=== Epoch: 59 ===\n","Avg Train Loss: 0.42280706763267517, Avg Train Acc: 0.8050000071525574\n","Avg Val Loss: 0.42443768456578257, Avg Val Acc: 0.8083333253860474 (Best: 0.8816666781902314)\n","=== Epoch: 60 ===\n","Avg Train Loss: 0.3388458780944347, Avg Train Acc: 0.8608333468437195\n","Avg Val Loss: 0.36336175156757233, Avg Val Acc: 0.85916668176651 (Best: 0.8816666781902314)\n","=== Epoch: 61 ===\n","Avg Train Loss: 0.43614284247159957, Avg Train Acc: 0.8200000077486038\n","Avg Val Loss: 0.5370258837938309, Avg Val Acc: 0.7616666674613952 (Best: 0.8816666781902314)\n","=== Epoch: 62 ===\n","Avg Train Loss: 0.4819162659347057, Avg Train Acc: 0.784166669845581\n","Avg Val Loss: 0.4398741997778416, Avg Val Acc: 0.8066666722297668 (Best: 0.8816666781902314)\n","=== Epoch: 63 ===\n","Avg Train Loss: 0.35454129315912725, Avg Train Acc: 0.8458333313465118\n","Avg Val Loss: 0.3452425915747881, Avg Val Acc: 0.8724999964237213 (Best: 0.8816666781902314)\n","=== Epoch: 64 ===\n","Avg Train Loss: 0.34034168124198916, Avg Train Acc: 0.8700000047683716\n","Avg Val Loss: 0.4777814939618111, Avg Val Acc: 0.8033333331346512 (Best: 0.8816666781902314)\n","=== Epoch: 65 ===\n","Avg Train Loss: 0.3508416496217251, Avg Train Acc: 0.8341666698455811\n","Avg Val Loss: 0.32805519737303257, Avg Val Acc: 0.8666666805744171 (Best: 0.8816666781902314)\n","=== Epoch: 66 ===\n","Avg Train Loss: 0.4798140197992325, Avg Train Acc: 0.8325000047683716\n","Avg Val Loss: 0.5240006573498249, Avg Val Acc: 0.7574999988079071 (Best: 0.8816666781902314)\n","=== Epoch: 67 ===\n","Avg Train Loss: 0.41428548395633696, Avg Train Acc: 0.8474999904632569\n","Avg Val Loss: 0.33658031933009624, Avg Val Acc: 0.8624999940395355 (Best: 0.8816666781902314)\n","=== Epoch: 68 ===\n","Avg Train Loss: 0.35924716644221916, Avg Train Acc: 0.8608333349227906\n","Avg Val Loss: 0.5066467002034187, Avg Val Acc: 0.7866666674613952 (Best: 0.8816666781902314)\n","=== Epoch: 69 ===\n","Avg Train Loss: 0.3142872853204608, Avg Train Acc: 0.8666666686534882\n","Avg Val Loss: 0.32683239206671716, Avg Val Acc: 0.8716666758060455 (Best: 0.8816666781902314)\n","=== Epoch: 70 ===\n","Avg Train Loss: 0.439110766351223, Avg Train Acc: 0.7941666603088379\n","Avg Val Loss: 0.29467457653954626, Avg Val Acc: 0.8624999940395355 (Best: 0.8816666781902314)\n","=== Epoch: 71 ===\n","Avg Train Loss: 0.31750044859945775, Avg Train Acc: 0.8524999976158142\n","Avg Val Loss: 0.4310278996825218, Avg Val Acc: 0.8066666662693024 (Best: 0.8816666781902314)\n","=== Epoch: 72 ===\n","Avg Train Loss: 0.310967843234539, Avg Train Acc: 0.8641666650772095\n","Avg Val Loss: 0.28440306037664415, Avg Val Acc: 0.8908333301544189 (Best)\n","=== Epoch: 73 ===\n","Avg Train Loss: 0.46060694605112074, Avg Train Acc: 0.7891666650772095\n","Avg Val Loss: 0.5314787358045578, Avg Val Acc: 0.7566666603088379 (Best: 0.8908333301544189)\n","=== Epoch: 74 ===\n","Avg Train Loss: 0.21034980416297913, Avg Train Acc: 0.8950000047683716\n","Avg Val Loss: 0.4077334351837635, Avg Val Acc: 0.8291666686534882 (Best: 0.8908333301544189)\n","=== Epoch: 75 ===\n","Avg Train Loss: 0.5450365729629993, Avg Train Acc: 0.7699999988079071\n","Avg Val Loss: 0.4157698415219784, Avg Val Acc: 0.7924999982118607 (Best: 0.8908333301544189)\n","=== Epoch: 76 ===\n","Avg Train Loss: 0.47784714326262473, Avg Train Acc: 0.7725000023841858\n","Avg Val Loss: 0.4071724198758602, Avg Val Acc: 0.8266666561365128 (Best: 0.8908333301544189)\n","=== Epoch: 77 ===\n","Avg Train Loss: 0.34396065161563455, Avg Train Acc: 0.8408333361148834\n","Avg Val Loss: 0.4788011111319065, Avg Val Acc: 0.8050000041723251 (Best: 0.8908333301544189)\n","=== Epoch: 78 ===\n","Avg Train Loss: 0.4165118411183357, Avg Train Acc: 0.8258333265781402\n","Avg Val Loss: 0.3614622041583061, Avg Val Acc: 0.8366666734218597 (Best: 0.8908333301544189)\n","=== Epoch: 79 ===\n","Avg Train Loss: 0.5255268722772598, Avg Train Acc: 0.753333330154419\n","Avg Val Loss: 0.3680960416793823, Avg Val Acc: 0.8458333373069763 (Best: 0.8908333301544189)\n","=== Epoch: 80 ===\n","Avg Train Loss: 0.31037537530064585, Avg Train Acc: 0.8849999964237213\n","Avg Val Loss: 0.3591926433146, Avg Val Acc: 0.8158333480358124 (Best: 0.8908333301544189)\n","=== Epoch: 81 ===\n","Avg Train Loss: 0.39987124586477873, Avg Train Acc: 0.8175000011920929\n","Avg Val Loss: 0.4803898110985756, Avg Val Acc: 0.7566666662693023 (Best: 0.8908333301544189)\n","=== Epoch: 82 ===\n","Avg Train Loss: 0.2529400667175651, Avg Train Acc: 0.9024999976158142\n","Avg Val Loss: 0.26124051250517366, Avg Val Acc: 0.8674999952316285 (Best: 0.8908333301544189)\n","=== Epoch: 83 ===\n","Avg Train Loss: 0.3038127214182168, Avg Train Acc: 0.8749999940395355\n","Avg Val Loss: 0.36646629869937897, Avg Val Acc: 0.8541666626930237 (Best: 0.8908333301544189)\n","=== Epoch: 84 ===\n","Avg Train Loss: 0.5117197781801224, Avg Train Acc: 0.7491666734218597\n","Avg Val Loss: 0.3128583297133446, Avg Val Acc: 0.8574999928474426 (Best: 0.8908333301544189)\n","=== Epoch: 85 ===\n","Avg Train Loss: 0.35115061700344086, Avg Train Acc: 0.8383333384990692\n","Avg Val Loss: 0.3979081593453884, Avg Val Acc: 0.8200000047683715 (Best: 0.8908333301544189)\n","=== Epoch: 86 ===\n","Avg Train Loss: 0.45381748359650376, Avg Train Acc: 0.8108333349227905\n","Avg Val Loss: 0.3088542852550745, Avg Val Acc: 0.8758333325386047 (Best: 0.8908333301544189)\n","=== Epoch: 87 ===\n","Avg Train Loss: 0.2573794642463326, Avg Train Acc: 0.901666671037674\n","Avg Val Loss: 0.4390346916392446, Avg Val Acc: 0.7983333349227906 (Best: 0.8908333301544189)\n","=== Epoch: 88 ===\n","Avg Train Loss: 0.4424362525343895, Avg Train Acc: 0.8050000071525574\n","Avg Val Loss: 0.4176194131374359, Avg Val Acc: 0.8399999976158142 (Best: 0.8908333301544189)\n","=== Epoch: 89 ===\n","Avg Train Loss: 0.29569973051548004, Avg Train Acc: 0.8733333289623261\n","Avg Val Loss: 0.3991425082087517, Avg Val Acc: 0.8258333384990693 (Best: 0.8908333301544189)\n","=== Epoch: 90 ===\n","Avg Train Loss: 0.44742861576378345, Avg Train Acc: 0.8233333349227905\n","Avg Val Loss: 0.39517067819833757, Avg Val Acc: 0.801666671037674 (Best: 0.8908333301544189)\n","=== Epoch: 91 ===\n","Avg Train Loss: 0.514189887046814, Avg Train Acc: 0.7800000011920929\n"],"name":"stdout"},{"output_type":"stream","text":["Avg Val Loss: 0.4872275717556477, Avg Val Acc: 0.8341666638851166 (Best: 0.8908333301544189)\n","=== Epoch: 92 ===\n","Avg Train Loss: 0.30118299648165703, Avg Train Acc: 0.8733333230018616\n","Avg Val Loss: 0.5928711771965027, Avg Val Acc: 0.7566666692495346 (Best: 0.8908333301544189)\n","=== Epoch: 93 ===\n","Avg Train Loss: 0.36218438670039177, Avg Train Acc: 0.8441666662693024\n","Avg Val Loss: 0.35378574058413503, Avg Val Acc: 0.8549999952316284 (Best: 0.8908333301544189)\n","=== Epoch: 94 ===\n","Avg Train Loss: 0.42437812946736814, Avg Train Acc: 0.8025000095367432\n","Avg Val Loss: 0.40537002980709075, Avg Val Acc: 0.8174999952316284 (Best: 0.8908333301544189)\n","=== Epoch: 95 ===\n","Avg Train Loss: 0.33703594356775285, Avg Train Acc: 0.8633333265781402\n","Avg Val Loss: 0.3377784192562103, Avg Val Acc: 0.8558333337306976 (Best: 0.8908333301544189)\n","=== Epoch: 96 ===\n","Avg Train Loss: 0.3021284569054842, Avg Train Acc: 0.8816666662693023\n","Avg Val Loss: 0.46581166684627534, Avg Val Acc: 0.7991666674613953 (Best: 0.8908333301544189)\n","=== Epoch: 97 ===\n","Avg Train Loss: 0.43974466770887377, Avg Train Acc: 0.810833340883255\n","Avg Val Loss: 0.26454682177864014, Avg Val Acc: 0.8824999928474426 (Best: 0.8908333301544189)\n","=== Epoch: 98 ===\n","Avg Train Loss: 0.30795382969081403, Avg Train Acc: 0.8625\n","Avg Val Loss: 0.27915181294083596, Avg Val Acc: 0.9033333361148834 (Best)\n","=== Epoch: 99 ===\n","Avg Train Loss: 0.3712441146373749, Avg Train Acc: 0.8408333361148834\n","Avg Val Loss: 0.371232458204031, Avg Val Acc: 0.8099999964237213 (Best: 0.9033333361148834)\n","Testing with last model..\n","Test Acc: 0.82325000166893\n","Testing with best model..\n","Test Acc: 0.8383333310484886\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q-X32lo3SZJo"},"source":[""],"execution_count":null,"outputs":[]}]}