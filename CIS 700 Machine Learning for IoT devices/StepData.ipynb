{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "waiseDF = pickle.load( open( \"waist_final_9306300.pkl\", \"rb\" ))\n",
    "wristDF = pickle.load( open( \"wrist_final_9891800.pkl\", \"rb\" ))\n",
    "otherDF = pickle.load( open( \"others.pkl\", \"rb\" ))\n",
    "drivingDF = pickle.load( open( \"dirving_final_4800000.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset class</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "        # Load data and get label\n",
    "        X = ID\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting X train and Y train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XLabel = wristDF[['x','y','z']]\n",
    "YLabel = wristDF[['walk']]\n",
    "xd = drivingDF[['x','y','z']]\n",
    "yd = drivingDF[['walk']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Reshapping the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = XLabel.values.reshape(98918,100,3)\n",
    "Y_train = YLabel.values.reshape(98918,100,1)\n",
    "Xd_train = xd.values.reshape(48000,100,3)\n",
    "Yd_train = yd.values.reshape(48000,100,1)\n",
    "XOther_train = otherDF\n",
    "YOther_train = np.full((48000,100,1),0)\n",
    "#Y_train = max(z) for z in Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 100, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 100, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xd_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate([X_train,Xd_train,XOther_train])\n",
    "Y_train = np.concatenate([Y_train,Yd_train,YOther_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training, validation and test split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(X_train, Y_train, test_size = 0.1)\n",
    "test_x, val_x, test_y, val_y = train_test_split(X_train, Y_train, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175426, 100, 3) (175426, 100, 1)\n",
      "(97459, 100, 3) (97459, 100, 1)\n",
      "(97459, 100, 3) (97459, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape)\n",
    "print(val_x.shape,val_y.shape)\n",
    "print(test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_x.reshape(175426,1,100,3)\n",
    "Y_train = train_y.reshape(175426,100)\n",
    "Y_train = np.array([max(z) for z in Y_train])\n",
    "\n",
    "X_val = val_x.reshape(97459,1,100,3)\n",
    "Y_val = val_y.reshape(97459,100)\n",
    "Y_val = np.array([max(z) for z in Y_val])\n",
    "\n",
    "X_test = test_x.reshape(97459,1,100,3)\n",
    "Y_test = test_y.reshape(97459,100)\n",
    "Y_test = np.array([max(z) for z in Y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(175426, 1, 100, 3) (175426,)\n",
      "(97459, 1, 100, 3) (97459,)\n",
      "(97459, 1, 100, 3) (97459,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_val.shape,Y_val.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([175426, 1, 100, 3]) torch.Size([175426])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.from_numpy(X_val)\n",
    "Y_val = torch.from_numpy(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97459, 1, 100, 3]) torch.Size([97459])\n"
     ]
    }
   ],
   "source": [
    "print(X_val.shape,Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(X_test)\n",
    "Y_test = torch.from_numpy(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97459, 1, 100, 3]) torch.Size([97459])\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,'num_workers': 6}\n",
    "max_epochs = 10\n",
    "\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(X_train, Y_train)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(X_val, Y_val)\n",
    "validation_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=(3,3), stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2,padding=1))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(100*2*4,64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=800, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "#learning_rate = 0.01\n",
    "if torch.cuda.is_available():\n",
    "    #model = nn.DataParallel(model)\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model, (1,100, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    #x_train, y_train = Variable(X_train), Variable(Y_train)\n",
    "    #x_val, y_val = Variable(X_val), Variable(Y_val)\n",
    "    for epoch in range(n_epochs):\n",
    "        for local_batch, local_labels in training_generator:\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "            \n",
    "            output_train = model(local_batch.float())\n",
    "            loss_train = criterion(output_train, local_labels)\n",
    "            train_losses.append(loss_train)\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss = loss_train.item()\n",
    "\n",
    "        print(tr_loss)\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_train)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "                local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "                \n",
    "                output_val = model(local_batch.float())\n",
    "                loss_val = criterion(output_val, local_labels)\n",
    "                val_losses.append(loss_val)\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4543533325195312e-05\n",
      "Epoch :  1 \t loss : tensor(1.4544e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  1 \t loss : tensor(4.8065, device='cuda:0')\n",
      "15.137739181518555\n",
      "Epoch :  2 \t loss : tensor(15.1377, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  2 \t loss : tensor(13.6008, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  3 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  3 \t loss : tensor(14.1870, device='cuda:0')\n",
      "0.0056716203689575195\n",
      "Epoch :  4 \t loss : tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  4 \t loss : tensor(1.9278, device='cuda:0')\n",
      "0.001661539077758789\n",
      "Epoch :  5 \t loss : tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  5 \t loss : tensor(2.4770, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  6 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  6 \t loss : tensor(7.7093, device='cuda:0')\n",
      "34.64246368408203\n",
      "Epoch :  7 \t loss : tensor(34.6425, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  7 \t loss : tensor(18.9830, device='cuda:0')\n",
      "8.106231689453125e-06\n",
      "Epoch :  8 \t loss : tensor(8.1062e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  8 \t loss : tensor(5.3714, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  9 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  9 \t loss : tensor(11.2916, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  10 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  10 \t loss : tensor(8.4778, device='cuda:0')\n",
      "4.934709072113037\n",
      "Epoch :  11 \t loss : tensor(4.9347, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  11 \t loss : tensor(2.5897, device='cuda:0')\n",
      "33.85406494140625\n",
      "Epoch :  12 \t loss : tensor(33.8541, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  12 \t loss : tensor(18.5204, device='cuda:0')\n",
      "43.283382415771484\n",
      "Epoch :  13 \t loss : tensor(43.2834, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  13 \t loss : tensor(23.7734, device='cuda:0')\n",
      "27.187641143798828\n",
      "Epoch :  14 \t loss : tensor(27.1876, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  14 \t loss : tensor(15.0038, device='cuda:0')\n",
      "0.00033283233642578125\n",
      "Epoch :  15 \t loss : tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  15 \t loss : tensor(3.5140, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  16 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  16 \t loss : tensor(15.2847, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  17 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  17 \t loss : tensor(19.8172, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  18 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  18 \t loss : tensor(18.7382, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  19 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  19 \t loss : tensor(11.8393, device='cuda:0')\n",
      "7.831839084625244\n",
      "Epoch :  20 \t loss : tensor(7.8318, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  20 \t loss : tensor(4.1794, device='cuda:0')\n",
      "38.82106018066406\n",
      "Epoch :  21 \t loss : tensor(38.8211, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  21 \t loss : tensor(21.2665, device='cuda:0')\n",
      "40.850830078125\n",
      "Epoch :  22 \t loss : tensor(40.8508, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  22 \t loss : tensor(22.4603, device='cuda:0')\n",
      "15.96790885925293\n",
      "Epoch :  23 \t loss : tensor(15.9679, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  23 \t loss : tensor(8.8686, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  24 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  24 \t loss : tensor(9.4452, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  25 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  25 \t loss : tensor(18.0835, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  26 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  26 \t loss : tensor(19.9737, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  27 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  27 \t loss : tensor(16.3382, device='cuda:0')\n",
      "3.337860107421875e-06\n",
      "Epoch :  28 \t loss : tensor(3.3379e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  28 \t loss : tensor(5.7649, device='cuda:0')\n",
      "25.526531219482422\n",
      "Epoch :  29 \t loss : tensor(25.5265, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  29 \t loss : tensor(13.9248, device='cuda:0')\n",
      "43.59852981567383\n",
      "Epoch :  30 \t loss : tensor(43.5985, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  30 \t loss : tensor(23.9292, device='cuda:0')\n",
      "31.91318130493164\n",
      "Epoch :  31 \t loss : tensor(31.9132, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  31 \t loss : tensor(17.5861, device='cuda:0')\n",
      "0.0949089527130127\n",
      "Epoch :  32 \t loss : tensor(0.0949, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  32 \t loss : tensor(1.0552, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  33 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  33 \t loss : tensor(14.2837, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  34 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  34 \t loss : tensor(19.6527, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  35 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  35 \t loss : tensor(19.0277, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  36 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  36 \t loss : tensor(12.5707, device='cuda:0')\n",
      "5.1365814208984375\n",
      "Epoch :  37 \t loss : tensor(5.1366, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  37 \t loss : tensor(2.7002, device='cuda:0')\n",
      "37.60187530517578\n",
      "Epoch :  38 \t loss : tensor(37.6019, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  38 \t loss : tensor(20.5915, device='cuda:0')\n",
      "41.62123107910156\n",
      "Epoch :  39 \t loss : tensor(41.6212, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  39 \t loss : tensor(22.8786, device='cuda:0')\n",
      "18.228870391845703\n",
      "Epoch :  40 \t loss : tensor(18.2289, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  40 \t loss : tensor(10.1053, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  41 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  41 \t loss : tensor(8.5837, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  42 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  42 \t loss : tensor(17.7601, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  43 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  43 \t loss : tensor(20.0204, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  44 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  44 \t loss : tensor(16.7322, device='cuda:0')\n",
      "4.76837158203125e-07\n",
      "Epoch :  45 \t loss : tensor(4.7684e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  45 \t loss : tensor(6.6815, device='cuda:0')\n",
      "23.486356735229492\n",
      "Epoch :  46 \t loss : tensor(23.4864, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  46 \t loss : tensor(12.8001, device='cuda:0')\n",
      "43.378475189208984\n",
      "Epoch :  47 \t loss : tensor(43.3785, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  47 \t loss : tensor(23.8035, device='cuda:0')\n",
      "33.42071533203125\n",
      "Epoch :  48 \t loss : tensor(33.4207, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  48 \t loss : tensor(18.4095, device='cuda:0')\n",
      "0.8665152788162231\n",
      "Epoch :  49 \t loss : tensor(0.8665, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  49 \t loss : tensor(0.7579, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  50 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  50 \t loss : tensor(13.6919, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  51 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  51 \t loss : tensor(19.4975, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  52 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  52 \t loss : tensor(19.2352, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  53 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  53 \t loss : tensor(13.2308, device='cuda:0')\n",
      "2.466392993927002\n",
      "Epoch :  54 \t loss : tensor(2.4664, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  54 \t loss : tensor(1.2889, device='cuda:0')\n",
      "36.220123291015625\n",
      "Epoch :  55 \t loss : tensor(36.2201, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  55 \t loss : tensor(19.8273, device='cuda:0')\n",
      "42.291969299316406\n",
      "Epoch :  56 \t loss : tensor(42.2920, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  56 \t loss : tensor(23.2420, device='cuda:0')\n",
      "20.468042373657227\n",
      "Epoch :  57 \t loss : tensor(20.4680, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  57 \t loss : tensor(11.3299, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  58 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  58 \t loss : tensor(7.6506, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  59 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  59 \t loss : tensor(17.3984, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  60 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  60 \t loss : tensor(20.0468, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  61 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  61 \t loss : tensor(17.1128, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  62 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  62 \t loss : tensor(7.5907, device='cuda:0')\n",
      "21.28028678894043\n",
      "Epoch :  63 \t loss : tensor(21.2803, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  63 \t loss : tensor(11.5844, device='cuda:0')\n",
      "43.035972595214844\n",
      "Epoch :  64 \t loss : tensor(43.0360, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  64 \t loss : tensor(23.6103, device='cuda:0')\n",
      "34.8463134765625\n",
      "Epoch :  65 \t loss : tensor(34.8463, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  65 \t loss : tensor(19.1878, device='cuda:0')\n",
      "2.996232748031616\n",
      "Epoch :  66 \t loss : tensor(2.9962, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  66 \t loss : tensor(1.7837, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  67 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  67 \t loss : tensor(13.0966, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  68 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  68 \t loss : tensor(19.3271, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  69 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  69 \t loss : tensor(19.4100, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  70 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  70 \t loss : tensor(13.8269, device='cuda:0')\n",
      "0.5768631100654602\n",
      "Epoch :  71 \t loss : tensor(0.5769, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  71 \t loss : tensor(0.6983, device='cuda:0')\n",
      "34.77717208862305\n",
      "Epoch :  72 \t loss : tensor(34.7772, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  72 \t loss : tensor(19.0297, device='cuda:0')\n",
      "42.817962646484375\n",
      "Epoch :  73 \t loss : tensor(42.8180, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  73 \t loss : tensor(23.5261, device='cuda:0')\n",
      "22.498905181884766\n",
      "Epoch :  74 \t loss : tensor(22.4989, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  74 \t loss : tensor(12.4405, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  75 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  75 \t loss : tensor(6.7322, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  76 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  76 \t loss : tensor(17.0239, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  77 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  77 \t loss : tensor(20.0516, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  78 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  78 \t loss : tensor(17.4575, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  79 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  79 \t loss : tensor(8.4315, device='cuda:0')\n",
      "19.053359985351562\n",
      "Epoch :  80 \t loss : tensor(19.0534, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  80 \t loss : tensor(10.3574, device='cuda:0')\n",
      "42.588623046875\n",
      "Epoch :  81 \t loss : tensor(42.5886, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  81 \t loss : tensor(23.3596, device='cuda:0')\n",
      "36.113929748535156\n",
      "Epoch :  82 \t loss : tensor(36.1139, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  82 \t loss : tensor(19.8796, device='cuda:0')\n",
      "5.374290466308594\n",
      "Epoch :  83 \t loss : tensor(5.3743, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  83 \t loss : tensor(3.0742, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  84 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  84 \t loss : tensor(12.5190, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  85 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  85 \t loss : tensor(19.1477, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  86 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  86 \t loss : tensor(19.5542, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  87 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  87 \t loss : tensor(14.3601, device='cuda:0')\n",
      "0.06046879291534424\n",
      "Epoch :  88 \t loss : tensor(0.0605, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  88 \t loss : tensor(1.4048, device='cuda:0')\n",
      "33.22472381591797\n",
      "Epoch :  89 \t loss : tensor(33.2247, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  89 \t loss : tensor(18.1722, device='cuda:0')\n",
      "43.227630615234375\n",
      "Epoch :  90 \t loss : tensor(43.2276, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  90 \t loss : tensor(23.7462, device='cuda:0')\n",
      "24.469566345214844\n",
      "Epoch :  91 \t loss : tensor(24.4696, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  91 \t loss : tensor(13.5180, device='cuda:0')\n",
      "2.384185791015625e-06\n",
      "Epoch :  92 \t loss : tensor(2.3842e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  92 \t loss : tensor(5.7410, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  93 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  93 \t loss : tensor(16.5895, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  94 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  94 \t loss : tensor(20.0334, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  95 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  95 \t loss : tensor(17.8103, device='cuda:0')\n",
      "0.0\n",
      "Epoch :  96 \t loss : tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  96 \t loss : tensor(9.3089, device='cuda:0')\n",
      "16.506195068359375\n",
      "Epoch :  97 \t loss : tensor(16.5062, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  97 \t loss : tensor(8.9545, device='cuda:0')\n",
      "41.95233154296875\n",
      "Epoch :  98 \t loss : tensor(41.9523, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  98 \t loss : tensor(23.0045, device='cuda:0')\n",
      "37.41097640991211\n",
      "Epoch :  99 \t loss : tensor(37.4110, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  99 \t loss : tensor(20.5871, device='cuda:0')\n",
      "7.9588775634765625\n",
      "Epoch :  100 \t loss : tensor(7.9589, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch :  100 \t loss : tensor(4.4875, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving the model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ConvNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/anaconda3/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"stepdatamodel_epoch100.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Visualization of train and validation loss</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py:128: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "#plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Generating training accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        output.append(model(local_batch.float().cuda()))\n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions = np.argmax(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.509913011754244"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_train, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Validation set accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in validation_generator:\n",
    "        output.append(model(local_batch.float().cuda()))\n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions = np.argmax(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5081726674806842"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_val, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Test set accuracy</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Dataset(X_test, Y_test)\n",
    "testset_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in testset_generator:\n",
    "        output.append(model(local_batch.float().cuda()))\n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions = np.argmax(temp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086036179316431"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
