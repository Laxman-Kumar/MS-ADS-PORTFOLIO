{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"Prototypical Learning.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"vk7oHVa5SebL"},"source":["from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import numpy as np\n","from torch.utils import data\n","import torch\n","from torchsummary import summary\n","import pickle\n","import os\n","from functions import prototypical_loss as loss_fn ,ProtoNet#,PrototypicalBatchSampler"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gXdOdszgSebN"},"source":["### Waist"]},{"cell_type":"code","metadata":{"id":"PN4bPbfrSebN","outputId":"9e7b6d84-688d-4eb9-9321-8fb65c52cf6f"},"source":["#waistDF = pickle.load( open( \"waist_final_10272800_new.pkl\", \"rb\" ))\n","waistDF = pickle.load( open( \"waist_final_10272800_new_nozero.pkl\", \"rb\" ))\n","\n","XTrain = waistDF[['x','y','z']]\n","YTrain = waistDF[['speed']]\n","\n","X_trainWaist = XTrain.values.reshape(102728,100,3)\n","Y_trainWaist = YTrain.values.reshape(102728,100,1)\n","Y_trainWaist = np.array([float(max(z)) for z in Y_trainWaist])\n","\n","waistTrain_x, waistVal_x, waistTrain_y, waistVal_y = train_test_split(X_trainWaist, Y_trainWaist, test_size = 0.1,random_state=100)\n","waistTest_x, waistVal_x, waistTest_y, waistVal_y = train_test_split(waistVal_x, waistVal_y, test_size = 0.5,random_state=100)\n","print(len(waistTrain_x))\n","print(len(waistTest_x))\n","print(len(waistVal_x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["92455\n","5136\n","5137\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yPXMEprySebP","outputId":"1e18ef85-f75b-45c8-d831-0ff09b89b20a"},"source":["waistTrain_x = waistTrain_x.reshape(92455,1,100,3)\n","waistTest_x = waistTest_x.reshape(5136,1,100,3)\n","waistVal_x = waistVal_x.reshape(5137,1,100,3)\n","print(waistTrain_x.shape)#,train_y.shape)\n","print(waistTest_x.shape)#,test_y.shape)\n","print(waistVal_x.shape)#,val_y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(92455, 1, 100, 3)\n","(5136, 1, 100, 3)\n","(5137, 1, 100, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"VjvRcB_USebP"},"source":["waistTrain_x = torch.from_numpy(waistTrain_x)\n","waistTrain_y = torch.from_numpy(waistTrain_y)\n","\n","waistTest_x = torch.from_numpy(waistTest_x)\n","waistTest_y = torch.from_numpy(waistTest_y)\n","\n","waistVal_x = torch.from_numpy(waistVal_x)\n","waistVal_y = torch.from_numpy(waistVal_y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fis762cnSebP"},"source":["### Wrist"]},{"cell_type":"code","metadata":{"id":"jouG_GONSebQ","outputId":"a5f15a58-ebd1-40f2-c371-7de6e6ed0fc4"},"source":["#wristDF = pickle.load( open( \"wrist_final_10272800_new.pkl\", \"rb\" ))\n","wristDF = pickle.load( open( \"wrist_final_10272800_new_nozero.pkl\", \"rb\" ))\n","\n","XTrain = wristDF[['x','y','z']]\n","YTrain = wristDF[['speed']]\n","\n","X_trainWrist = XTrain.values.reshape(102728,100,3)\n","Y_trainWrist = YTrain.values.reshape(102728,100,1)\n","Y_trainWrist = np.array([float(max(z)) for z in Y_trainWrist])\n","\n","wristTrain_x, wristVal_x, wristTrain_y, wristVal_y = train_test_split(X_trainWrist, Y_trainWrist, test_size = 0.1,random_state=100)\n","wristTest_x, wristVal_x, wristTest_y, wristVal_y = train_test_split(wristVal_x, wristVal_y, test_size = 0.5,random_state=100)\n","print(len(wristTrain_x))\n","print(len(wristTest_x))\n","print(len(wristVal_x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["92455\n","5136\n","5137\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1uXsED9bSebQ","outputId":"fa201752-69f3-4bd3-936d-dfb5520f821a"},"source":["wristTrain_x[0].shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 3)"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"yFNKs1LySebQ","outputId":"35898a61-aceb-4d61-e67a-e79881c9ed34"},"source":["wristTrain_x = wristTrain_x.reshape(92455,1,100,3)\n","wristTest_x = wristTest_x.reshape(5136,1,100,3)\n","wristVal_x = wristVal_x.reshape(5137,1,100,3)\n","print(wristTrain_x.shape)#,train_y.shape)\n","print(wristTest_x.shape)#,test_y.shape)\n","print(wristVal_x.shape)#,val_y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(92455, 1, 100, 3)\n","(5136, 1, 100, 3)\n","(5137, 1, 100, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TkjwYa0hSebR"},"source":["wristTrain_x = torch.from_numpy(wristTrain_x)\n","wristTrain_y = torch.from_numpy(wristTrain_y)\n","\n","wristTest_x = torch.from_numpy(wristTest_x)\n","wristTest_y = torch.from_numpy(wristTest_y)\n","\n","wristVal_x = torch.from_numpy(wristVal_x)\n","wristVal_y = torch.from_numpy(wristVal_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vqm5ciSkSebR","outputId":"d3bc351f-c142-47ea-cf43-de7d521693e0"},"source":["#Classes\n","print(len(np.unique(wristTrain_y)),len(np.unique(wristTest_y)),len(np.unique(wristVal_y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["14 14 14\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"19gECI0zSebR"},"source":["### DataSet Class"]},{"cell_type":"code","metadata":{"id":"lNAtZ_vQSebS"},"source":["class Dataset(data.Dataset):\n","    def __init__(self, X1,Y):\n","        'Initialization'\n","        self.X1 = X1\n","        self.Y = Y\n","        #self.transform = transforms.Compose([transforms.ToTensor()])\n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.X1)\n","    \n","    def __getitem__(self, index):\n","        'Generates one sample of data'\n","        # Select sample\n","        x = self.X1[index]\n","        #x = self.transform(x)\n","        y = self.Y[index]\n","        #y = self.transform(y)\n","        return x,y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hS7x3SdnSebS"},"source":["training_set = Dataset(wristTrain_x, wristTrain_y)\n","test_set = Dataset(wristTest_x, wristTest_y)\n","val_set = Dataset(wristVal_x, wristVal_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-sBGxLnlSebS"},"source":["x = ['dataset_root','experiment_root','cuda']\n","\n","classes_per_it_tr = 3\n","classes_per_it_val = 3\n","classes_per_it_tr = 3\n","\n","manual_seeds =7\n","iterations = 10\n","epochs = 100\n","learning_rate = 0.01\n","lr_scheduler_step = 0.1\n","lr_scheduler_gamma = 0.5\n","cuda = 0\n","num_support_tr = 40\n","num_query_tr = 40\n","num_support_val = 40\n","num_query_val = 40"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i6DCDlHESebT"},"source":["class PrototypicalBatchSampler(object):\n","    '''\n","    PrototypicalBatchSampler: yield a batch of indexes at each iteration.\n","    Indexes are calculated by keeping in account 'classes_per_it' and 'num_samples',\n","    In fact at every iteration the batch indexes will refer to  'num_support' + 'num_query' samples\n","    for 'classes_per_it' random classes.\n","\n","    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n","    '''\n","\n","    def __init__(self, labels, classes_per_it, num_samples, iterations):\n","        '''\n","        Initialize the PrototypicalBatchSampler object\n","        Args:\n","        - labels: an iterable containing all the labels for the current dataset\n","        samples indexes will be infered from this iterable.\n","        - classes_per_it: number of random classes for each iteration\n","        - num_samples: number of samples for each iteration for each class (support + query)\n","        - iterations: number of iterations (episodes) per epoch\n","        '''\n","        super(PrototypicalBatchSampler, self).__init__()\n","        self.labels = labels\n","        self.labels = [float(x) for x in self.labels]\n","        self.classes_per_it = classes_per_it\n","        self.sample_per_class = num_samples\n","        self.iterations = iterations\n","\n","        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n","        self.classes = [float(x) for x in self.classes]\n","        self.classes = torch.FloatTensor(self.classes)\n","        #print(self.classes)\n","        # create a matrix, indexes, of dim: classes X max(elements per class)\n","        # fill it with nans\n","        # for every class c, fill the relative row with the indices samples belonging to c\n","        # in numel_per_class we store the number of samples for each class/row\n","        self.idxs = range(len(self.labels))\n","        self.indexes = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n","        self.indexes = torch.Tensor(self.indexes)\n","        self.numel_per_class = torch.zeros_like(self.classes)\n","        for idx, label in enumerate(self.labels):\n","\n","            label_idx = np.argwhere(self.classes == label).item()\n","            self.indexes[label_idx, np.where(np.isnan(self.indexes[label_idx]))[0][0]] = idx\n","            self.numel_per_class[label_idx] += 1\n","\n","    def __iter__(self):\n","        '''\n","        yield a batch of indexes\n","        '''\n","        \n","        spc = self.sample_per_class\n","        cpi = self.classes_per_it\n","        \n","        #print(spc)\n","        for it in range(self.iterations):\n","            batch_size = spc * cpi\n","            batch = torch.LongTensor(batch_size)\n","            c_idxs = torch.randperm(len(self.classes))[:cpi]\n","            for i, c in enumerate(self.classes[c_idxs]):\n","                s = slice(i * spc, (i + 1) * spc)\n","                # FIXME when torch.argwhere will exists\n","                label_idx = torch.arange(len(self.classes)).long()[self.classes == c].item()\n","                sample_idxs = torch.randperm(int(self.numel_per_class[label_idx]))[:spc]\n","                batch[s] = self.indexes[label_idx][sample_idxs]\n","            batch = batch[torch.randperm(len(batch))]\n","            yield batch\n","\n","    def __len__(self):\n","        '''\n","        returns the number of iterations (episodes) per epoch\n","        '''\n","        return self.iterations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqwve7Q2SebU"},"source":["def init_seed(manual_seed):\n","    '''\n","    Disable cudnn to maximize reproducibility\n","    '''\n","    torch.cuda.cudnn_enabled = False\n","    np.random.seed(manual_seed)\n","    torch.manual_seed(manual_seed)\n","    torch.cuda.manual_seed(manual_seed)\n","    \n","def init_dataset(mode):\n","    if (mode == \"train\"):dataset = training_set\n","    if (mode == \"test\"):dataset = test_set\n","    if (mode == \"val\"):dataset = val_set\n","    n_classes = 14\n","    if n_classes < classes_per_it_tr or n_classes < classes_per_it_val:\n","        raise(Exception('There are not enough classes in the dataset in order ' +\n","                        'to satisfy the chosen classes_per_it. Decrease the ' +\n","                        'classes_per_it_{tr/val} option and try again.'))\n","    return dataset\n","\n","def init_sampler(labels, mode):\n","    if 'train' in mode:\n","        classes_per_it = classes_per_it_tr\n","        num_samples = num_support_tr + num_query_tr\n","    else:\n","        classes_per_it = classes_per_it_val\n","        num_samples = num_support_val + num_query_val\n","\n","    return PrototypicalBatchSampler(labels=labels,classes_per_it=classes_per_it,\n","                                    num_samples=num_samples,\n","                                    iterations=iterations)\n","\n","def init_dataloader(mode):\n","    dataset = init_dataset(mode)\n","    sampler = init_sampler(dataset.Y, mode)\n","    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n","    return dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FJuqDsgGSebU"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J64JkQuVSebU"},"source":["def init_protonet():\n","    device = 'cuda:0' if torch.cuda.is_available() and cuda else 'cpu'\n","    model = ProtoNet().to(device)\n","    return model\n","\n","def init_optim(model):return torch.optim.Adam(params=model.parameters(),lr=learning_rate)\n","\n","def init_lr_scheduler(optim):\n","    return torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=lr_scheduler_gamma,step_size=lr_scheduler_step)\n","\n","def save_list_to_file(path, thelist):\n","    with open(path, 'w') as f:\n","        for item in thelist:f.write(\"%s\\n\" % item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDe79wfbSebV"},"source":["def train(tr_dataloader, model, optim, lr_scheduler, val_dataloader=None):\n","    \n","    device = 'cuda:0' if torch.cuda.is_available() and cuda else 'cpu'\n","\n","    if val_dataloader is None:best_state = None\n","    \n","    train_loss = []\n","    train_acc = []\n","    val_loss = []\n","    val_acc = []\n","    best_acc = 0\n","\n","    best_model_path = 'best_model.pth'\n","    last_model_path = 'last_model.pth'\n","\n","    for epoch in range(epochs):\n","        print('=== Epoch: {} ==='.format(epoch))\n","        tr_iter = iter(tr_dataloader)\n","        model.train()\n","        for batch in tr_iter:\n","            optim.zero_grad()\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            model_output = model(x.float())\n","            loss,acc= loss_fn(model_output,target=y,n_support=num_support_tr)\n","            loss.backward()\n","            optim.step()\n","            train_loss.append(loss.item())\n","            train_acc.append(acc.item())\n","        avg_loss = np.mean(train_loss[-iterations:])\n","        avg_acc = np.mean(train_acc[-iterations:])\n","        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n","        lr_scheduler.step()\n","        if val_dataloader is None:\n","            continue\n","            \n","        val_iter = iter(val_dataloader)\n","        model.eval()\n","        \n","        for batch in val_iter:\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            model_output = model(x.float())\n","            loss, acc = loss_fn(model_output, target=y,n_support=num_support_val)\n","            val_loss.append(loss.item())\n","            val_acc.append(acc.item())\n","        avg_loss = np.mean(val_loss[-iterations:])\n","        avg_acc = np.mean(val_acc[-iterations:])\n","        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(best_acc)\n","        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(avg_loss, avg_acc, postfix))\n","        if avg_acc >= best_acc:\n","            torch.save(model.state_dict(), best_model_path)\n","            best_acc = avg_acc\n","            best_state = model.state_dict()\n","\n","    torch.save(model.state_dict(), last_model_path)\n","\n","    for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n","        save_list_to_file(os.path.join(name + '.txt'), locals()[name])\n","\n","    return best_state, best_acc, train_loss, train_acc, val_loss, val_acc\n","\n","from sklearn.metrics import confusion_matrix\n","def test(test_dataloader, model):\n","    '''\n","    Test the model trained with the prototypical learning algorithm\n","    '''\n","    device = 'cuda:0' if torch.cuda.is_available() and cuda else 'cpu'\n","    avg_acc = list()\n","    for epoch in range(10):\n","        test_iter = iter(test_dataloader)\n","        for batch in test_iter:\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","            model_output = model(x.float())\n","            _, acc = loss_fn(model_output, target=y,n_support=num_support_val)\n","            avg_acc.append(acc.item())\n","    avg_acc = np.mean(avg_acc)\n","    print('Test Acc: {}'.format(avg_acc))\n","    \n","    \n","    return avg_acc\n","\n","\n","def eval():\n","    '''\n","    Initialize everything and train\n","    '''\n","    #options = get_parser().parse_args()\n","\n","    if torch.cuda.is_available() and not options.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","    init_seed()\n","    test_dataloader = init_dataset()[-1]\n","    model = init_protonet()\n","    model_path = os.path.join('best_model.pth')\n","    model.load_state_dict(torch.load(model_path))\n","\n","    test(opt=options,test_dataloader=test_dataloader,model=model)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GADmvbQYSebW"},"source":["def main():\n","  \n","    if torch.cuda.is_available() and not cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","    init_seed(manual_seeds)\n","    tr_dataloader = init_dataloader('train')\n","    val_dataloader = init_dataloader('val')\n","    # trainval_dataloader = init_dataloader(options, 'trainval')\n","    test_dataloader = init_dataloader('test')\n","\n","    model = init_protonet()\n","    optim = init_optim(model)\n","    \n","    lr_scheduler = init_lr_scheduler(optim)\n","    res = train(tr_dataloader=tr_dataloader,val_dataloader=val_dataloader,lr_scheduler=lr_scheduler,\n","                model=model,optim=optim)\n","    \n","    best_state, best_acc, train_loss, train_acc, val_loss, val_acc = res\n","    print('Testing with last model..')\n","    test(test_dataloader=test_dataloader,model=model)\n","\n","    model.load_state_dict(best_state)\n","    print('Testing with best model..')\n","    test(test_dataloader=test_dataloader,model=model)\n","\n","    # optim = init_optim(options, model)\n","    # lr_scheduler = init_lr_scheduler(options, optim)\n","\n","    # print('Training on train+val set..')\n","    # train(opt=options,tr_dataloader=trainval_dataloader,\n","    #       val_dataloader=None,model=model,optim=optim,lr_scheduler=lr_scheduler)\n","\n","    # print('Testing final model..')\n","    # test(opt=options,test_dataloader=test_dataloader,model=model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-GsoIzoSebW","outputId":"09c75257-4232-49b1-c83b-2f2f7223ef97"},"source":["classes_per_it_tr = 3\n","classes_per_it_val = 3\n","classes_per_it_tr = 3\n","\n","iterations = 10\n","epochs = 100\n","\n","num_support_tr = 40\n","num_query_tr = 40\n","num_support_val = 40\n","num_query_val = 40\n","\n","main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["=== Epoch: 0 ===\n","Avg Train Loss: 12.35082654953003, Avg Train Acc: 0.7058333247900009\n","Avg Val Loss: 11.192837452888488, Avg Val Acc: 0.715833330154419 (Best)\n","=== Epoch: 1 ===\n","Avg Train Loss: 3.433455228805542, Avg Train Acc: 0.6624999970197678\n","Avg Val Loss: 2.515294075012207, Avg Val Acc: 0.6750000059604645 (Best: 0.715833330154419)\n","=== Epoch: 2 ===\n","Avg Train Loss: 1.281914359331131, Avg Train Acc: 0.6691666632890702\n","Avg Val Loss: 1.6508618652820588, Avg Val Acc: 0.6683333337306976 (Best: 0.715833330154419)\n","=== Epoch: 3 ===\n","Avg Train Loss: 0.9590822562575341, Avg Train Acc: 0.6974999845027924\n","Avg Val Loss: 0.994181650876999, Avg Val Acc: 0.676666659116745 (Best: 0.715833330154419)\n","=== Epoch: 4 ===\n","Avg Train Loss: 0.660728533565998, Avg Train Acc: 0.7483333349227905\n","Avg Val Loss: 0.8035464286804199, Avg Val Acc: 0.6816666662693024 (Best: 0.715833330154419)\n","=== Epoch: 5 ===\n","Avg Train Loss: 0.6505081921815872, Avg Train Acc: 0.7266666650772095\n","Avg Val Loss: 0.6443431794643402, Avg Val Acc: 0.7199999988079071 (Best)\n","=== Epoch: 6 ===\n","Avg Train Loss: 0.6888300150632858, Avg Train Acc: 0.7366666734218598\n","Avg Val Loss: 0.5655016243457794, Avg Val Acc: 0.7683333277702331 (Best)\n","=== Epoch: 7 ===\n","Avg Train Loss: 0.6999521791934967, Avg Train Acc: 0.7125\n","Avg Val Loss: 0.6485666036605835, Avg Val Acc: 0.7208333373069763 (Best: 0.7683333277702331)\n","=== Epoch: 8 ===\n","Avg Train Loss: 0.5733486443758011, Avg Train Acc: 0.7716666698455811\n","Avg Val Loss: 0.5349186837673188, Avg Val Acc: 0.7683333337306977 (Best)\n","=== Epoch: 9 ===\n","Avg Train Loss: 0.6471118181943893, Avg Train Acc: 0.7266666650772095\n","Avg Val Loss: 0.541312438249588, Avg Val Acc: 0.764166671037674 (Best: 0.7683333337306977)\n","=== Epoch: 10 ===\n","Avg Train Loss: 0.49315630793571474, Avg Train Acc: 0.776666671037674\n","Avg Val Loss: 0.7005474835634231, Avg Val Acc: 0.6958333253860474 (Best: 0.7683333337306977)\n","=== Epoch: 11 ===\n","Avg Train Loss: 0.5192255586385727, Avg Train Acc: 0.7541666686534881\n","Avg Val Loss: 0.5767502278089524, Avg Val Acc: 0.7608333319425583 (Best: 0.7683333337306977)\n","=== Epoch: 12 ===\n","Avg Train Loss: 0.6089262574911117, Avg Train Acc: 0.7449999958276748\n","Avg Val Loss: 0.5750208720564842, Avg Val Acc: 0.7166666597127914 (Best: 0.7683333337306977)\n","=== Epoch: 13 ===\n","Avg Train Loss: 0.564869050681591, Avg Train Acc: 0.7491666674613953\n","Avg Val Loss: 0.5639685600996017, Avg Val Acc: 0.7466666579246521 (Best: 0.7683333337306977)\n","=== Epoch: 14 ===\n","Avg Train Loss: 0.7404680550098419, Avg Train Acc: 0.6500000029802322\n","Avg Val Loss: 0.5670195817947388, Avg Val Acc: 0.7549999952316284 (Best: 0.7683333337306977)\n","=== Epoch: 15 ===\n","Avg Train Loss: 0.40841385424137117, Avg Train Acc: 0.809166669845581\n","Avg Val Loss: 0.5734513401985168, Avg Val Acc: 0.7341666758060456 (Best: 0.7683333337306977)\n","=== Epoch: 16 ===\n","Avg Train Loss: 0.47741694897413256, Avg Train Acc: 0.7733333349227905\n","Avg Val Loss: 0.5407225102186203, Avg Val Acc: 0.7241666734218597 (Best: 0.7683333337306977)\n","=== Epoch: 17 ===\n","Avg Train Loss: 0.5701783850789071, Avg Train Acc: 0.7458333373069763\n","Avg Val Loss: 0.620746573805809, Avg Val Acc: 0.7224999904632569 (Best: 0.7683333337306977)\n","=== Epoch: 18 ===\n","Avg Train Loss: 0.5245834469795227, Avg Train Acc: 0.7658333331346512\n","Avg Val Loss: 0.5857250988483429, Avg Val Acc: 0.6950000017881394 (Best: 0.7683333337306977)\n","=== Epoch: 19 ===\n","Avg Train Loss: 0.46649051904678346, Avg Train Acc: 0.7983333230018616\n","Avg Val Loss: 0.5582529872655868, Avg Val Acc: 0.7483333349227905 (Best: 0.7683333337306977)\n","=== Epoch: 20 ===\n","Avg Train Loss: 0.499755734205246, Avg Train Acc: 0.775\n","Avg Val Loss: 0.5823854684829712, Avg Val Acc: 0.7274999976158142 (Best: 0.7683333337306977)\n","=== Epoch: 21 ===\n","Avg Train Loss: 0.5953549802303314, Avg Train Acc: 0.7475000083446502\n","Avg Val Loss: 0.5539193153381348, Avg Val Acc: 0.751666659116745 (Best: 0.7683333337306977)\n","=== Epoch: 22 ===\n","Avg Train Loss: 0.5809984326362609, Avg Train Acc: 0.7049999952316284\n","Avg Val Loss: 0.6497461140155792, Avg Val Acc: 0.6925000011920929 (Best: 0.7683333337306977)\n","=== Epoch: 23 ===\n","Avg Train Loss: 0.5067126169800759, Avg Train Acc: 0.7699999988079071\n","Avg Val Loss: 0.5118598401546478, Avg Val Acc: 0.7449999988079071 (Best: 0.7683333337306977)\n","=== Epoch: 24 ===\n","Avg Train Loss: 0.5592755734920501, Avg Train Acc: 0.7583333373069763\n","Avg Val Loss: 0.6449294805526733, Avg Val Acc: 0.7066666722297669 (Best: 0.7683333337306977)\n","=== Epoch: 25 ===\n","Avg Train Loss: 0.5319405362010002, Avg Train Acc: 0.7625\n","Avg Val Loss: 0.5248878791928291, Avg Val Acc: 0.7658333331346512 (Best: 0.7683333337306977)\n","=== Epoch: 26 ===\n","Avg Train Loss: 0.4472662717103958, Avg Train Acc: 0.7841666579246521\n","Avg Val Loss: 0.4971271112561226, Avg Val Acc: 0.7791666746139526 (Best)\n","=== Epoch: 27 ===\n","Avg Train Loss: 0.5393942877650261, Avg Train Acc: 0.764166671037674\n","Avg Val Loss: 0.4877469763159752, Avg Val Acc: 0.750833335518837 (Best: 0.7791666746139526)\n","=== Epoch: 28 ===\n","Avg Train Loss: 0.6175367414951325, Avg Train Acc: 0.710000005364418\n","Avg Val Loss: 0.6188489764928817, Avg Val Acc: 0.7325000047683716 (Best: 0.7791666746139526)\n","=== Epoch: 29 ===\n","Avg Train Loss: 0.42987436056137085, Avg Train Acc: 0.8074999928474427\n","Avg Val Loss: 0.5277894034981727, Avg Val Acc: 0.7575000047683715 (Best: 0.7791666746139526)\n","=== Epoch: 30 ===\n","Avg Train Loss: 0.5228440314531326, Avg Train Acc: 0.7708333313465119\n","Avg Val Loss: 0.5690242052078247, Avg Val Acc: 0.7258333325386047 (Best: 0.7791666746139526)\n","=== Epoch: 31 ===\n","Avg Train Loss: 0.6034252569079399, Avg Train Acc: 0.7349999934434891\n","Avg Val Loss: 0.5157842487096786, Avg Val Acc: 0.7816666543483735 (Best)\n","=== Epoch: 32 ===\n","Avg Train Loss: 0.4102691516280174, Avg Train Acc: 0.8050000011920929\n","Avg Val Loss: 0.5583437293767929, Avg Val Acc: 0.7241666674613952 (Best: 0.7816666543483735)\n","=== Epoch: 33 ===\n","Avg Train Loss: 0.5464561611413956, Avg Train Acc: 0.7466666728258133\n","Avg Val Loss: 0.47012520730495455, Avg Val Acc: 0.7958333373069764 (Best)\n","=== Epoch: 34 ===\n","Avg Train Loss: 0.4036146983504295, Avg Train Acc: 0.8150000095367431\n","Avg Val Loss: 0.5805973976850509, Avg Val Acc: 0.7533333361148834 (Best: 0.7958333373069764)\n","=== Epoch: 35 ===\n","Avg Train Loss: 0.5281775832176209, Avg Train Acc: 0.7366666674613953\n","Avg Val Loss: 0.5220592767000198, Avg Val Acc: 0.7633333325386047 (Best: 0.7958333373069764)\n","=== Epoch: 36 ===\n","Avg Train Loss: 0.636299905180931, Avg Train Acc: 0.7116666674613953\n","Avg Val Loss: 0.5126548737287522, Avg Val Acc: 0.7491666704416275 (Best: 0.7958333373069764)\n","=== Epoch: 37 ===\n","Avg Train Loss: 0.5463574707508088, Avg Train Acc: 0.7133333325386048\n","Avg Val Loss: 0.567381677031517, Avg Val Acc: 0.7266666680574417 (Best: 0.7958333373069764)\n","=== Epoch: 38 ===\n","Avg Train Loss: 0.533532838523388, Avg Train Acc: 0.7374999970197678\n","Avg Val Loss: 0.5824557632207871, Avg Val Acc: 0.6974999904632568 (Best: 0.7958333373069764)\n","=== Epoch: 39 ===\n","Avg Train Loss: 0.5155933052301407, Avg Train Acc: 0.7700000047683716\n","Avg Val Loss: 0.5387940526008606, Avg Val Acc: 0.7574999928474426 (Best: 0.7958333373069764)\n","=== Epoch: 40 ===\n","Avg Train Loss: 0.43188208639621734, Avg Train Acc: 0.7933333396911622\n","Avg Val Loss: 0.5504907906055451, Avg Val Acc: 0.7449999988079071 (Best: 0.7958333373069764)\n","=== Epoch: 41 ===\n","Avg Train Loss: 0.4112638235092163, Avg Train Acc: 0.8233333349227905\n","Avg Val Loss: 0.6666022926568985, Avg Val Acc: 0.6775000035762787 (Best: 0.7958333373069764)\n","=== Epoch: 42 ===\n","Avg Train Loss: 0.5500031784176826, Avg Train Acc: 0.7633333384990693\n","Avg Val Loss: 0.5459224998950958, Avg Val Acc: 0.7400000035762787 (Best: 0.7958333373069764)\n","=== Epoch: 43 ===\n","Avg Train Loss: 0.6430620774626732, Avg Train Acc: 0.6816666692495346\n","Avg Val Loss: 0.5743208318948746, Avg Val Acc: 0.7566666722297668 (Best: 0.7958333373069764)\n","=== Epoch: 44 ===\n","Avg Train Loss: 0.426178640127182, Avg Train Acc: 0.7808333337306976\n","Avg Val Loss: 0.5124860614538193, Avg Val Acc: 0.7699999988079071 (Best: 0.7958333373069764)\n","=== Epoch: 45 ===\n","Avg Train Loss: 0.5108245223760605, Avg Train Acc: 0.7699999958276749\n","Avg Val Loss: 0.6150526016950607, Avg Val Acc: 0.7166666686534882 (Best: 0.7958333373069764)\n","=== Epoch: 46 ===\n","Avg Train Loss: 0.5939195483922959, Avg Train Acc: 0.7266666650772095\n"],"name":"stdout"},{"output_type":"stream","text":["Avg Val Loss: 0.4875902786850929, Avg Val Acc: 0.7641666650772094 (Best: 0.7958333373069764)\n","=== Epoch: 47 ===\n","Avg Train Loss: 0.54975795596838, Avg Train Acc: 0.7191666573286056\n","Avg Val Loss: 0.5627656936645508, Avg Val Acc: 0.7758333444595337 (Best: 0.7958333373069764)\n","=== Epoch: 48 ===\n","Avg Train Loss: 0.4843359425663948, Avg Train Acc: 0.7800000071525574\n","Avg Val Loss: 0.5986810445785522, Avg Val Acc: 0.7308333426713943 (Best: 0.7958333373069764)\n","=== Epoch: 49 ===\n","Avg Train Loss: 0.6215250194072723, Avg Train Acc: 0.6974999934434891\n","Avg Val Loss: 0.5033434100449086, Avg Val Acc: 0.7991666674613953 (Best)\n","=== Epoch: 50 ===\n","Avg Train Loss: 0.5302128896117211, Avg Train Acc: 0.7599999964237213\n","Avg Val Loss: 0.4796666204929352, Avg Val Acc: 0.7599999964237213 (Best: 0.7991666674613953)\n","=== Epoch: 51 ===\n","Avg Train Loss: 0.5116166219115257, Avg Train Acc: 0.7800000011920929\n","Avg Val Loss: 0.4634395450353622, Avg Val Acc: 0.8041666626930237 (Best)\n","=== Epoch: 52 ===\n","Avg Train Loss: 0.46278217285871504, Avg Train Acc: 0.7908333361148834\n","Avg Val Loss: 0.5219356864690781, Avg Val Acc: 0.7524999946355819 (Best: 0.8041666626930237)\n","=== Epoch: 53 ===\n","Avg Train Loss: 0.47710389345884324, Avg Train Acc: 0.7941666543483734\n","Avg Val Loss: 0.5115644216537476, Avg Val Acc: 0.7624999940395355 (Best: 0.8041666626930237)\n","=== Epoch: 54 ===\n","Avg Train Loss: 0.5564170330762863, Avg Train Acc: 0.748333340883255\n","Avg Val Loss: 0.3879206359386444, Avg Val Acc: 0.83416668176651 (Best)\n","=== Epoch: 55 ===\n","Avg Train Loss: 0.5212089031934738, Avg Train Acc: 0.7508333325386047\n","Avg Val Loss: 0.5121194958686829, Avg Val Acc: 0.7733333289623261 (Best: 0.83416668176651)\n","=== Epoch: 56 ===\n","Avg Train Loss: 0.37795313745737075, Avg Train Acc: 0.8416666686534882\n","Avg Val Loss: 0.6135692536830902, Avg Val Acc: 0.7291666626930237 (Best: 0.83416668176651)\n","=== Epoch: 57 ===\n","Avg Train Loss: 0.601894634962082, Avg Train Acc: 0.7258333325386047\n","Avg Val Loss: 0.5087101936340332, Avg Val Acc: 0.7775000035762787 (Best: 0.83416668176651)\n","=== Epoch: 58 ===\n","Avg Train Loss: 0.6626625031232833, Avg Train Acc: 0.688333335518837\n","Avg Val Loss: 0.4459033399820328, Avg Val Acc: 0.798333328962326 (Best: 0.83416668176651)\n","=== Epoch: 59 ===\n","Avg Train Loss: 0.4919818788766861, Avg Train Acc: 0.759166669845581\n","Avg Val Loss: 0.6398387596011161, Avg Val Acc: 0.686666664481163 (Best: 0.83416668176651)\n","=== Epoch: 60 ===\n","Avg Train Loss: 0.6449167639017105, Avg Train Acc: 0.6975000083446503\n","Avg Val Loss: 0.6374403238296509, Avg Val Acc: 0.7050000011920929 (Best: 0.83416668176651)\n","=== Epoch: 61 ===\n","Avg Train Loss: 0.4876865416765213, Avg Train Acc: 0.753333330154419\n","Avg Val Loss: 0.6112663894891739, Avg Val Acc: 0.7208333313465118 (Best: 0.83416668176651)\n","=== Epoch: 62 ===\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n_fi04N4SebW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1ojh0saSebW"},"source":[""],"execution_count":null,"outputs":[]}]}