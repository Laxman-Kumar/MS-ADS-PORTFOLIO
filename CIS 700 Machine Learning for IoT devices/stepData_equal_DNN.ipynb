{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam, SGD\n",
    "from torchsummary import summary\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "waistDF = pickle.load( open( \"waist_final_10272800_new.pkl\", \"rb\" ))\n",
    "wristDF = pickle.load( open( \"wrist_final_10272800_new.pkl\", \"rb\" ))\n",
    "#otherDF = pickle.load( open( \"others.pkl\", \"rb\" ))\n",
    "wristdrivingDF = pickle.load( open( \"equal_wrist_dirving_final_4800000.pkl\", \"rb\" ) )\n",
    "waistdrivingDF = pickle.load( open( \"equal_waist_dirving_final_4800000.pkl\", \"rb\" ) )\n",
    "wristothersDF = pickle.load( open( \"equal_wrist_others_final_10220000.pkl\", \"rb\" ) )\n",
    "waistothersDF = pickle.load( open( \"equal_waist_others_final_10220000.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# </h3>Dataset class</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "   \n",
    "    def __init__(self, X1,X2,Y):\n",
    "        'Initialization'\n",
    "        self.X1 = X1\n",
    "        self.X2 = X2\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X1)\n",
    "   \n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        x1 = self.X1[index]\n",
    "        x2 = self.X2[index]\n",
    "        y = self.Y[index]\n",
    "\n",
    "        return x1,x2,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Extracting X train and Y train</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XWrist = wristDF[['x','y','z']]\n",
    "YWrist = wristDF[['walk']]\n",
    "XWaist = waistDF[['x','y','z']]\n",
    "YWaist = waistDF[['walk']]\n",
    "\n",
    "XWaistDriving = waistdrivingDF[['x','y','z']]\n",
    "YWaistDriving = waistdrivingDF[['walk']]\n",
    "XWristDriving = wristdrivingDF[['x','y','z']]\n",
    "YWristDriving = wristdrivingDF[['walk']]\n",
    "\n",
    "XWaistothers = waistothersDF[['x','y','z']]\n",
    "YWaistothers = waistothersDF[['walk']]\n",
    "XWristothers = wristothersDF[['x','y','z']]\n",
    "YWristothers = wristothersDF[['walk']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10140000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(XWaistDriving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###preprocessing data -1 to 1 ###\n",
    "XWrist = preprocessing.normalize(XWrist.values)\n",
    "#YWrist = wristDF[['walk']]\n",
    "XWaist = preprocessing.normalize(XWaist.values)\n",
    "#YWaist = waistDF[['walk']]\n",
    "\n",
    "XWaistDriving = preprocessing.normalize(XWaistDriving.values)\n",
    "#YWaistDriving = waistdrivingDF[['walk']]\n",
    "XWristDriving = preprocessing.normalize(XWristDriving.values)\n",
    "#YWristDriving = wristdrivingDF[['walk']]\n",
    "\n",
    "XWaistothers = preprocessing.normalize(XWaistothers.values)\n",
    "#YWaistothers = waistothersDF[['walk']]\n",
    "XWristothers = preprocessing.normalize(XWristothers.values)\n",
    "#YWristothers = wristothersDF[['walk']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3>Reshapping the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = XLabel.values.reshape(98918,100,3)\n",
    "#Y_train = YLabel.values.reshape(98918,100,1)\n",
    "XWrist_train = XWrist.reshape(102728,100,3)\n",
    "YWrist_train = np.full((102728,100,3),[1,0,0])\n",
    "XWaist_train = XWaist.reshape(102728,100,3)\n",
    "YWaist_train = np.full((102728,100,3),[1,0,0])\n",
    "XWristDriving_train = XWristDriving.reshape(101400,100,3)\n",
    "YWristDriving_train = np.full((101400,100,3),[0,1,0])\n",
    "XWaistDriving_train = XWaistDriving.reshape(101400,100,3)\n",
    "YWaistDriving_train = np.full((101400,100,3),[0,1,0])\n",
    "XWristothers_train = XWristothers.reshape(102200,100,3)\n",
    "YWristothers_train = np.full((102200,100,3),[0,0,1])\n",
    "XWaistothers_train = XWaistothers.reshape(102200,100,3)\n",
    "YWaistothers_train = np.full((102200,100,3),[0,0,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(YWaistothers_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_waist = np.concatenate([XWrist_train,XWristDriving_train,XWristothers_train])\n",
    "Y_train_waist = np.concatenate([YWrist_train,YWristDriving_train,YWristothers_train])\n",
    "X_train_wrist = np.concatenate([XWaist_train,XWaistDriving_train,XWaistothers_train])\n",
    "Y_train_wrist = np.concatenate([YWaist_train,YWaistDriving_train,YWaistothers_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306328, 100, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_waist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306328, 100, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_waist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306328, 100, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_wrist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306328, 100, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_wrist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306328"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_waist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combine=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(X_train_waist)):\n",
    "    temp = []\n",
    "    temp.append(X_train_waist[i])\n",
    "    temp.append(X_train_wrist[i])\n",
    "    X_train_combine.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_combine = np.array(X_train_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_combine=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(Y_train_waist)):\n",
    "    temp = []\n",
    "    temp.append(Y_train_waist[i])\n",
    "    temp.append(Y_train_wrist[i])\n",
    "    Y_train_combine.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_combine = np.array(Y_train_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306328, 100, 3)\n",
      "(306328, 100, 3)\n",
      "(306328, 2, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_waist.shape)\n",
    "print(Y_train_waist.shape)\n",
    "print(Y_train_combine.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3>Training, validation and test split</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91899, 2, 100, 3) (91899, 2, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "combine_train_x, combine_val_x, combine_y, combine_val_y = train_test_split(X_train_combine, Y_train_combine, test_size = 0.3,train_size=0.7)\n",
    "print(combine_val_x.shape,combine_val_y.shape)\n",
    "combine_test_x, combine_val_x, combine_test_y, combine_val_y = train_test_split(combine_val_x, combine_val_y, test_size = 2/3,train_size=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214429, 2, 100, 3) (214429, 2, 100, 3)\n",
      "(61266, 2, 100, 3) (61266, 2, 100, 3)\n",
      "(30633, 2, 100, 3) (30633, 2, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(combine_train_x.shape,combine_y.shape)\n",
    "print(combine_val_x.shape,combine_val_y.shape)\n",
    "print(combine_test_x.shape,combine_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "waist_train_x=[] \n",
    "waist_train_y=[]\n",
    "waist_val_x=[]\n",
    "waist_val_y=[]\n",
    "waist_test_x=[]\n",
    "waist_test_y=[]\n",
    "wrist_train_x=[] \n",
    "wrist_train_y=[]\n",
    "wrist_val_x=[]\n",
    "wrist_val_y=[]\n",
    "wrist_test_x=[]\n",
    "wrist_test_y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(combine_train_x)):\n",
    "    #temp = []\n",
    "    waist_train_x.append(combine_train_x[i][0])\n",
    "    wrist_train_x.append(combine_train_x[i][1])\n",
    "    \n",
    "for i in range (len(combine_y)):\n",
    "    #temp = []\n",
    "    waist_train_y.append(combine_y[i][0])\n",
    "    wrist_train_y.append(combine_y[i][1])\n",
    "    \n",
    "    \n",
    "for i in range (len(combine_val_x)):\n",
    "    #temp = []\n",
    "    waist_val_x.append(combine_val_x[i][0])\n",
    "    wrist_val_x.append(combine_val_x[i][1])\n",
    "    \n",
    "for i in range (len(combine_val_y)):\n",
    "    #temp = []\n",
    "    waist_val_y.append(combine_val_y[i][0])\n",
    "    wrist_val_y.append(combine_val_y[i][1])\n",
    "    \n",
    "    \n",
    "for i in range (len(combine_test_x)):\n",
    "    #temp = []\n",
    "    waist_test_x.append(combine_test_x[i][0])\n",
    "    wrist_test_x.append(combine_test_x[i][1])\n",
    "    \n",
    "for i in range (len(combine_test_y)):\n",
    "    #temp = []\n",
    "    waist_test_y.append(combine_test_y[i][0])\n",
    "    wrist_test_y.append(combine_test_y[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "waist_train_x=np.array(waist_train_x)\n",
    "waist_train_y=np.array(waist_train_y)\n",
    "waist_val_x=np.array(waist_val_x)\n",
    "waist_val_y=np.array(waist_val_y)\n",
    "waist_test_x=np.array(waist_test_x)\n",
    "waist_test_y=np.array(waist_test_y)\n",
    "wrist_train_x=np.array(wrist_train_x)\n",
    "wrist_train_y=np.array(wrist_train_y)\n",
    "wrist_val_x=np.array(wrist_val_x)\n",
    "wrist_val_y=np.array(wrist_val_y)\n",
    "wrist_test_x=np.array(wrist_test_x)\n",
    "wrist_test_y=np.array(wrist_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214429, 100, 3) (214429, 100, 3)\n",
      "(61266, 100, 3) (61266, 100, 3)\n",
      "(30633, 100, 3) (30633, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(waist_train_x.shape,waist_train_y.shape)\n",
    "print(waist_val_x.shape,waist_val_y.shape)\n",
    "print(waist_test_x.shape,waist_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214429, 100, 3) (214429, 100, 3)\n",
      "(61266, 100, 3) (61266, 100, 3)\n",
      "(30633, 100, 3) (30633, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "print(wrist_train_x.shape,wrist_train_y.shape)\n",
    "print(wrist_val_x.shape,wrist_val_y.shape)\n",
    "print(wrist_test_x.shape,wrist_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_trainWaist[80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainWaist = waist_train_x.reshape(214429,1,100,3)\n",
    "Y_trainWaist = waist_train_y.reshape(214429,100,3)\n",
    "temp=[]\n",
    "for z in Y_trainWaist:\n",
    "    temp.append(z[0])\n",
    "Y_trainWaist = np.array(temp)\n",
    "\n",
    "\n",
    "X_valWaist = waist_val_x.reshape(61266,1,100,3)\n",
    "Y_valWaist = waist_val_y.reshape(61266,100,3)\n",
    "temp=[]\n",
    "for z in Y_valWaist:\n",
    "    temp.append(z[0])\n",
    "Y_valWaist = np.array(temp)\n",
    "\n",
    "\n",
    "\n",
    "#X_test = test_x.reshape(97459,1,100,3)\n",
    "#Y_test = test_y.reshape(97459,100)\n",
    "X_testWaist = waist_test_x.reshape(30633,1,100,3)\n",
    "Y_testWaist = waist_test_y.reshape(30633,100,3)\n",
    "temp=[]\n",
    "for z in Y_testWaist:\n",
    "    temp.append(z[0])\n",
    "Y_testWaist = np.array(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214429, 1, 100, 3) (214429, 3)\n",
      "(61266, 1, 100, 3) (61266, 3)\n",
      "(30633, 1, 100, 3) (30633, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_trainWaist.shape,Y_trainWaist.shape)\n",
    "print(X_valWaist.shape,Y_valWaist.shape)\n",
    "print(X_testWaist.shape,Y_testWaist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainWrist = wrist_train_x.reshape(214429,1,100,3)\n",
    "Y_trainWrist = wrist_train_y.reshape(214429,100,3)\n",
    "temp=[]\n",
    "for z in Y_trainWrist:\n",
    "    temp.append(z[0])\n",
    "Y_trainWrist = np.array(temp)\n",
    "\n",
    "\n",
    "X_valWrist = wrist_val_x.reshape(61266,1,100,3)\n",
    "Y_valWrist = wrist_val_y.reshape(61266,100,3)\n",
    "temp=[]\n",
    "for z in Y_valWrist:\n",
    "    temp.append(z[0])\n",
    "Y_valWrist = np.array(temp)\n",
    "\n",
    "\n",
    "\n",
    "#X_test = test_x.reshape(97459,1,100,3)\n",
    "#Y_test = test_y.reshape(97459,100)\n",
    "X_testWrist = waist_test_x.reshape(30633,1,100,3)\n",
    "Y_testWrist = waist_test_y.reshape(30633,100,3)\n",
    "temp=[]\n",
    "for z in Y_testWrist:\n",
    "    temp.append(z[0])\n",
    "Y_testWrist = np.array(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_trainWrist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_trainWaist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214429, 1, 100, 3) (214429, 3)\n",
      "(61266, 1, 100, 3) (61266, 3)\n",
      "(30633, 1, 100, 3) (30633, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_trainWrist.shape,Y_trainWrist.shape)\n",
    "print(X_valWrist.shape,Y_valWrist.shape)\n",
    "print(X_testWrist.shape,Y_testWrist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'splitdata/X_trainWrist_equal5.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-f569a086ce8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'splitdata/X_trainWrist_equal5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_trainWrist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'splitdata/Y_train_equal5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_trainWrist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'splitdata/X_valWrist_equal5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valWrist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'splitdata/Y_valWrist_equal5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valWrist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'splitdata/X_testWrist_equal5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_testWrist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msave\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\Python\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.npy'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'splitdata/X_trainWrist_equal5.npy'"
     ]
    }
   ],
   "source": [
    "np.save('splitdata/X_trainWrist_equal5', X_trainWrist) \n",
    "np.save('splitdata/Y_train_equal5', Y_trainWrist) \n",
    "np.save('splitdata/X_valWrist_equal5', X_valWrist) \n",
    "np.save('splitdata/Y_valWrist_equal5', Y_valWrist) \n",
    "np.save('splitdata/X_testWrist_equal5', X_testWrist) \n",
    "np.save('splitdata/Y_testWrist_equal5', Y_testWrist) \n",
    "\n",
    "\n",
    "np.save('splitdata/X_trainWaist_equal5', X_trainWaist) \n",
    "np.save('splitdata/Y_trainWaist_equal5', Y_trainWaist) \n",
    "np.save('splitdata/X_valWaist_equal5', X_valWaist) \n",
    "np.save('splitdata/Y_valWaist_equal5', Y_valWaist) \n",
    "np.save('splitdata/X_testWaist_equal5', X_testWaist) \n",
    "np.save('splitdata/Y_testWaist_equal5', Y_testWaist) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3>Tensors</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainWaist = torch.from_numpy(X_trainWaist)\n",
    "Y_trainWaist = torch.from_numpy(Y_trainWaist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainWrist = torch.from_numpy(X_trainWrist)\n",
    "Y_trainWrist = torch.from_numpy(Y_trainWrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trainWaist.shape,Y_trainWaist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trainWrist.shape,Y_trainWrist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valWaist = torch.from_numpy(X_valWaist)\n",
    "Y_valWaist = torch.from_numpy(Y_valWaist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valWaist.shape,Y_valWaist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valWrist = torch.from_numpy(X_valWrist)\n",
    "Y_valWrist = torch.from_numpy(Y_valWrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_valWrist.shape,Y_valWrist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testWaist = torch.from_numpy(X_testWaist)\n",
    "Y_testWaist = torch.from_numpy(Y_testWaist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_testWaist.shape,Y_testWaist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testWrist = torch.from_numpy(X_testWrist)\n",
    "Y_testWrist = torch.from_numpy(Y_testWrist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30633, 1, 100, 3) (30633, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_testWrist.shape,Y_testWrist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214429, 1, 100, 3)\n",
      "(214429, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_trainWaist.shape)\n",
    "print(Y_trainWaist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 256,'num_workers': 6}\n",
    "#params = {'batch_size': 256,'num_workers': 6}\n",
    "max_epochs = 10\n",
    "\n",
    "\n",
    "# Generators\n",
    "training_setCombine = Dataset(X_trainWaist, X_trainWrist, Y_trainWaist)\n",
    "training_generatorCombine = data.DataLoader(training_setCombine, **params)\n",
    "\n",
    "validation_setCombine = Dataset(X_valWaist,X_valWrist, Y_valWaist)\n",
    "validation_generatorCombine = data.DataLoader(validation_setCombine, **params)\n",
    "\n",
    "test_set = Dataset(X_testWaist,X_testWrist, Y_testWaist)\n",
    "testset_generatorCombine = data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_setCombine.__getitem__(8)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=training_setCombine.__getitem__(8)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h3>Building model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetWaist(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNetWaist, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(100*2*4,64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "        self.fc1 = nn.Linear(100*2*4,64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        #out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        #out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetWrist(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(ConvNetWrist, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=(3,3), stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            #nn.Dropout(p=0.9),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "            \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(kernel_size=2, stride=2,padding=1))\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2,padding=1))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=(3,3), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(p=0.8),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        #self.fc1 = nn.Linear(100*2*4,64)\n",
    "        #self.fc2 = nn.Linear(64,2)\n",
    "        #self.fc1 = nn.Linear(100*2*4,64)\n",
    "        #self.fc2 = nn.Linear(64,2)\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        #out = self.layer4(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        #out = F.relu(self.fc1(out))\n",
    "        #out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combine_Waist_Wrist(nn.Module):\n",
    "    def __init__(self, modelA, modelB):\n",
    "        super(Combine_Waist_Wrist, self).__init__()\n",
    "        self.modelA = modelA\n",
    "        self.modelB = modelB\n",
    "        \n",
    "        self.fc1 = nn.Linear(100*2*4*2,64)\n",
    "        #self.fc1 = nn.Linear(100*2*2,64)\n",
    "        \n",
    "        self.classifier = nn.Linear(64,3)   ### 2 -> 1 , make labels 1 dimensions: 0 or 1\n",
    "        #self.softmax = nn.Softmax(dim=3)\n",
    "        #linear layers\n",
    "        #linear layers\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.modelA(x1)\n",
    "        x2 = self.modelB(x2)\n",
    "        out = torch.cat((x1, x2), dim=1)\n",
    "        \n",
    "        out = F.relu(self.fc1(out))\n",
    "        #out = (self.fc1(out))\n",
    "        x = self.classifier(out)  # not relu\n",
    "        #x = F.tanh(out)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "modelWaist = ConvNetWaist()\n",
    "modelWrist = ConvNetWrist()\n",
    "modelCombine = Combine_Waist_Wrist(modelWaist,modelWrist)\n",
    "#model=model.Dropout(p=0.2)\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizerWaist = Adam(modelWaist.parameters(), lr=0.02)\n",
    "#optimizerWrist = Adam(modelWrist.parameters(), lr=0.02)\n",
    "#optimizerCombine = optimizerWaist +optimizerWrist\n",
    "\n",
    "params = list(modelWaist.parameters()) + list(modelWrist.parameters())\n",
    "optimizerCombine = Adam(params, lr=0.0001)\n",
    "scheduler = StepLR(optimizerCombine, step_size=1, gamma=0.9)\n",
    "#weight_decay=1e-5\n",
    "\n",
    "#optimizerCombine = SGD(modelCombine.parameters(), lr=0.001)\n",
    "#optimizer = SGD(model.parameters(), lr=0.01)\n",
    "#learning_rate = 0.01\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    #model = nn.DataParallel(model)\n",
    "    modelWaist = modelWaist.cuda()\n",
    "    modelWrist = modelWrist.cuda()\n",
    "    modelCombine = modelCombine.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "modelWaist.to(device)\n",
    "modelWrist.to(device)\n",
    "modelCombine.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(modelCombine, [(1,100, 3),(1,100, 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### how it will look like with new dataloader\n",
    "\n",
    "n_epochs = 50\n",
    "# empty list to store training losses\n",
    "#train_losses_waist = []\n",
    "#train_losses_wrist = []\n",
    "train_losses_combine = []\n",
    "train_losses_combine_avg = []\n",
    "\n",
    "# empty list to store validation losses\n",
    "#val_losses_waist = []\n",
    "#val_losses_wrist = []\n",
    "val_losses_combine = []\n",
    "val_losses_combine_avg = []\n",
    "\n",
    "\n",
    "def train():\n",
    "    modelCombine.train()\n",
    "    tr_loss = 0\n",
    "    \n",
    "    #n_epochs = 10\n",
    "    best_loss=99999999999999\n",
    "    for epoch in range(n_epochs):\n",
    "        tr_loss = 0\n",
    "        val_loss = 0\n",
    "        #scheduler.step()\n",
    "        temp1=[]\n",
    "        temp2=[]\n",
    "        for local_batch_waist, local_batch_wrist, local_labels in training_generatorCombine:\n",
    "            local_batch_waist, local_batch_wrist, local_labels = local_batch_waist.to(device), local_batch_wrist.to(device), local_labels.to(device)\n",
    "            \n",
    "            optimizerCombine.zero_grad()\n",
    "            #output_train = modelCombine(local_batch_waist.float(), local_batch_wrist.float())\n",
    "            output_train = modelCombine(local_batch_waist.float(), local_batch_wrist.float())\n",
    "            \n",
    "            #print(output_train.size())\n",
    "            #print(local_labels.float().size())\n",
    "            #print(output_train)\n",
    "            #print(torch.argmax(output_train, 1))\n",
    "            #print(torch.argmax(local_labels.float(), 1))\n",
    "            #print(local_labels.float().squeeze().size())\n",
    "            ##print(output_train)\n",
    "            #print(local_labels.float.uniform_(0, 1))\n",
    "            #loss_train = criterion(torch.argmax(output_train, 1), torch.argmax(local_labels.float(), 1))\n",
    "            loss_train = criterion(output_train, local_labels.squeeze(1).float())\n",
    "            \n",
    "            #print(loss_train)\n",
    "\n",
    "            temp1.append(loss_train.item())\n",
    "            train_losses_combine.append(loss_train.item())\n",
    "            \n",
    "            loss_train.backward()\n",
    "            optimizerCombine.step()\n",
    "            #tr_loss = loss_train.item()\n",
    "            #tr_loss = tr_loss + loss_train.item()\n",
    "        avg_loss = np.mean(temp1)\n",
    "        train_losses_combine_avg.append(avg_loss)\n",
    "        \n",
    "        print(tr_loss)\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', avg_loss)\n",
    "        #print('Epoch : ',epoch+1, '\\t', 'loss :', tr_loss)\n",
    "        \n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            for local_batch_waist, local_batch_wrist, local_labels in validation_generatorCombine:\n",
    "                local_batch_waist, local_batch_wrist, local_labels = local_batch_waist.to(device), local_batch_wrist.to(device), local_labels.to(device)\n",
    "                \n",
    "                \n",
    "                \n",
    "                output_val = modelCombine(local_batch_waist.float(), local_batch_wrist.float())\n",
    "                #print(output_val.size())\n",
    "                #print(local_labels.float().size())\n",
    "                #print(output_val.unsqueeze(1).size())\n",
    "                #print(local_labels.float().unsqueeze(1).size())\n",
    "                \n",
    "                loss_val = criterion(output_val, local_labels.float())\n",
    "                val_losses_combine.append(loss_val.item())\n",
    "                temp2.append(loss_val.item())\n",
    "                \n",
    "                #val_loss = val_loss + loss_val\n",
    "            avg_loss = np.mean(temp2)\n",
    "            if (avg_loss<=best_loss):\n",
    "                #torch.save(model.state_dict(), best_model_path)\n",
    "                torch.save(modelCombine.state_dict(), \"seventh_equal_best.pt\")\n",
    "                best_loss=avg_loss\n",
    "                print(\"best:  \",avg_loss)\n",
    "                # safe the best mode -> lowest validation loss\n",
    "                \n",
    "            \n",
    "            val_losses_combine_avg.append(avg_loss)\n",
    "            print('Epoch : ',epoch+1, '\\t', 'loss :', avg_loss)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(modelCombine.state_dict(), \"seventh_equal_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(modelCombine.state_dict(), \"Second_equal_last.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelC = init_protonet()\n",
    "'''\n",
    "modelWaist1 = ConvNetWaist()#\n",
    "modelWrist1 = ConvNetWrist()\n",
    "modelCombine1 = Combine_Waist_Wrist(modelWaist1,modelWrist1)\n",
    "modelWaist1.to(device)\n",
    "modelWrist1.to(device)\n",
    "modelCombine1.to(device)\n",
    "modelCombine1.load_state_dict(torch.load(\"Second_equal_last.pt\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_combine_avg, label='Training loss')\n",
    "plt.plot(val_losses_combine_avg, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('Equal7_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses_combine, label='Training loss')\n",
    "plt.plot(val_losses_combine, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('Equal7.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelCombine = torch.load(\"1Combine_step_model_dropout0.9_weightdecay_scheduler_epoch20_equal.pt\")\n",
    "#modelCombine.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for local_batch_waist, local_batch_wrist, local_labels in testset_generatorCombine:\n",
    "        print(local_labels)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction for training set\n",
    "output =[]\n",
    "prediction=[]\n",
    "count=0\n",
    "with torch.no_grad():\n",
    "    for local_batch_waist, local_batch_wrist, local_labels in testset_generatorCombine:\n",
    "        count=count+1\n",
    "            #local_batch_waist, local_batch_wrist, local_labels = local_batch_waist.to(device), local_batch_wrist.to(device), local_labels.to(device)\n",
    "        a=modelCombine(local_batch_waist.float().cuda(), local_batch_wrist.float().cuda())\n",
    "        #print(a)\n",
    "        output.append(a)\n",
    "        #output.append(modelWaist(local_batch.float().cuda()))\n",
    "        prediction.append(local_labels)\n",
    "        \n",
    "    print(count)        \n",
    "\"\"\"    \n",
    "temp =[]\n",
    "for x in output:\n",
    "    softmax = torch.exp(x).cpu()\n",
    "    prob = list(softmax.numpy())\n",
    "    temp.extend(prob)\n",
    "predictions3 = np.argmax(temp, axis=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = open(\"splitdata/outputequatest6_1.pkl\", 'wb')\n",
    "pickle.dump(output, loss1)\n",
    "loss1.close()\n",
    "loss2 = open(\"splitdata/predictionequaltest6_1.pkl\", 'wb')\n",
    "pickle.dump(prediction, loss2)\n",
    "loss2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0=0\n",
    "count_1=0\n",
    "count_2=0\n",
    "rcount_0=0\n",
    "rcount_1=0\n",
    "rcount_2=0\n",
    "should1_but0=0\n",
    "should1_but2=0\n",
    "should2_but0=0\n",
    "should2_but1=0\n",
    "should0_but1=0\n",
    "should0_but2=0\n",
    "\n",
    "for i in range(0,(len(output))):\n",
    "    pred=output[i]\n",
    "    labelset=prediction[i]\n",
    "    #print(i)\n",
    "    for j in range(0,len(pred)):\n",
    "        if(torch.argmax(labelset[j].cpu()).item()==0):\n",
    "            count_0=count_0+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==0):\n",
    "                rcount_0=rcount_0+1   \n",
    "            if(torch.argmax(pred[j].cpu()).item()==1):\n",
    "                should0_but1=should0_but1+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==2):\n",
    "                should0_but2=should0_but2+1\n",
    "        #else:\n",
    "            \n",
    "        elif(torch.argmax(labelset[j].cpu()).item()==1):\n",
    "            count_1=count_1+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==1):\n",
    "                rcount_1=rcount_1+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==0):\n",
    "                should1_but0=should1_but0+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==2):\n",
    "                should1_but2=should1_but2+1\n",
    "\n",
    "        elif(torch.argmax(labelset[j].cpu()).item()==2):\n",
    "            count_2=count_2+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==2):\n",
    "                rcount_2=rcount_2+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==0):\n",
    "                should2_but0=should2_but0+1\n",
    "            if(torch.argmax(pred[j].cpu()).item()==1):\n",
    "                should2_but1=should2_but1+1    \n",
    "        #if torch.argmax(output[j].cpu()).item() == torch.argmax(prediction[j].cpu()).item() ==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(count_0,rcount_0)\n",
    "print(count_1,rcount_1)\n",
    "print(count_2,rcount_2)\n",
    "print(should2_but0)\n",
    "print(should2_but1)\n",
    "print(should1_but0)\n",
    "print(should1_but2)\n",
    "print(should0_but1)\n",
    "print(should0_but2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
